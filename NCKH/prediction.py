# -*- coding: utf-8 -*-
"""[1] HEALTH_INSURANCE_CROSS_SELL_PREDICTION.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1FN_PJ5dEsWV65882HyX5Gl_SgbjBFjTA

# I. IMPORT AND OVERVIEW THE DATA

## 1.1 Import Required Libraries
"""

# Basic Libraries
import pandas as pd
import numpy as np
import time
import pickle
import optuna

# Plotation Libraries
import matplotlib.pyplot as plt
from matplotlib.pylab import random_sample
import seaborn as sns
sns.set(style='whitegrid')
import plotly.express as px

# Preprocessing Libraries
from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, RobustScaler
from sklearn.feature_selection import mutual_info_classif

# Machine Learning Models
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
import xgboost as xgb

# Model Evaluation Metrics
from sklearn.metrics import (
    accuracy_score, confusion_matrix, f1_score, roc_curve, roc_auc_score,
    precision_score, recall_score, log_loss, precision_recall_curve, auc,
    classification_report
)

# Handling Imbalanced Data
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import NearMiss

# Hyperparameter Tuning
from sklearn.experimental import enable_halving_search_cv
from sklearn.model_selection import (
    KFold, train_test_split, cross_val_score, StratifiedKFold,
    RandomizedSearchCV, GridSearchCV, HalvingRandomSearchCV
)
from hyperopt import STATUS_OK, Trials, fmin, hp, tpe

# Misc
from sklearn.metrics import make_scorer
from joblib import Parallel, delayed

from optuna.samplers import TPESampler, RandomSampler

"""## 1.2 Import Data"""

# Import Original Data
train_org = pd.read_csv("train.csv")
test_org = pd.read_csv("test.csv")

# Duplicated the data in case if the duplicated data has change, there's no change in original data
train = train_org.copy()
test = test_org.copy()

"""### 1.2.1 Over view the train dataset"""

train.describe()

train.head()

train.info()

train_record, train_feature = train.shape
print(f"The Train Dataset have: {train_record} records \nThe Train Dataset have: {train_feature} features")

# Checking for duplicated data
train_duplicated = train.duplicated()
train_duplicated.value_counts()

"""---
After reviewing the train dataset, we can draw some conclusions as follows:
- The train set contains **381109 records**, equivalent to 381109 rows.
- The train set has **12 features**, equivalent to 12 columns, including one target column named Response.
- The train set does not have any null values or duplicated values.
- The data columns in the train set are currently in the correct format, and no adjustments are needed.
- Some columns contain values that fall under categorical variables, for example, the columns Gender, Driving_License, Previously_Insured, Vehicle_Age, Vehicle_Damage, and Response.

### 1.2.2 Over view the Test dataset
"""

test.describe()

test.info()

test.head()

test_record, test_feature = test.shape
print(f"The Train Dataset have: {test_record} records \nThe Train Dataset have: {test_feature} features")

# Checking for duplicated data
test_duplicated = test.duplicated()
test_duplicated.value_counts()

"""---


After reviewing the test dataset, we can draw some conclusions as follows:
- The test set contains **127037 records**, equivalent to 127037 rows.
- The train set has **11 features**, equivalent to 11 columns.
- The train set does not have any null values or duplicated values.
- The data columns in the test set are currently in the correct format, and no adjustments are needed.
- Some columns contain values that fall under categorical variables, for example, the columns Gender, Driving_License, Previously_Insured, Vehicle_Age, Vehicle_Damage.

# II. DATA VISUALIZATION

## 2.1 Univariate Charts:

### 2.1.1 ID

The ID column, which consists of incrementing values serving as identifiers, is currently irrelevant to the processes of visualization, Exploratory Data Analysis (EDA), and model training. Therefore, it is appropriate to drop the ID column from the train dataset. This action will help focus on the features that potentially influence the model's predictions.
"""

train.drop('id', axis =1, inplace=True)

"""### 2.1.2 Gender"""

train['Gender'].value_counts()

sns.catplot(x="Gender",
                data=train, kind="count",
                height=4, aspect=.9);
plt.title('Distribution of Gender')
plt.xticks([0,1], ['0:Male', '1:Female'])

"""- Male are likely to buy health insunrance than Female

### 2.1.3 Age
"""

# Create a countplot with KDE for 'Age'
sns.set(style="whitegrid")
ax = sns.histplot(data=train, x='Age', kde=True, color='skyblue')

# Add a vertical line for the mean age
mean_age = train['Age'].mean()
ax.axvline(mean_age, color='red', linestyle='--', label=f'Mean Age: {mean_age:.2f}')

# Set plot title and labels
plt.title("Age Distribution of Customers")
plt.xlabel("Age")
plt.ylabel("Count")

# Add legend
plt.legend()

# Show the plot
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(train['Age'])
plt.title('Boxplot of Age')
plt.xlabel('Age')
plt.show()

"""- Customer from 20-30 and 40-50 are likely to buy health insurance

### 2.1.4 Driving_License
"""

sns.countplot(x = 'Driving_License', data = train)
plt.title('Driving License Distribution')
plt.xticks([0,1], [ 'Do not Have', 'Already Have'])
plt.xlabel('Driving License')
plt.ylabel('Count')

Do_Not_Have, Already_Have = train['Driving_License'].value_counts()
print(f'There are {Do_Not_Have} customer already have Driving License \nThere are {Already_Have} customer do not have Driving License')

"""- So all the customer who have buy Health Insurance mostly have Driving License

### 2.1.5 Region Code
"""

train['Region_Code'].value_counts().count()

train['Region_Code'].plot(kind='hist', bins=53, title='Region_Code')
plt.gca().spines[['top', 'right',]].set_visible(False)

plt.figure(figsize=(10, 6))
sns.boxplot(train['Region_Code'])
plt.title('Boxplot of Region Code')
plt.xlabel('Region Code')
plt.show()

"""- So we can see that region code 28 buy the most health insurance.

### 2.1.6 Previously_Insured
"""

train['Previously_Insured'].value_counts()

sns.countplot(x ='Previously_Insured', data = train)
plt.xticks([0,1], ['No', 'Yes'])
plt.title('Distribution of Previously_Insured')
plt.xlabel('Previously_Insured')
plt.ylabel('Count')
plt.show()

"""- We can see that the number of people who have previously purchased Vehicle Insurance is relatively high, but still fewer than the customers who have never purchased Vehicle Insurance in the past

### 2.1.7 Vehicle Age
"""

train['Vehicle_Age'].value_counts()

sns.catplot(x="Vehicle_Age",
                data=train, kind="count",
                height=4, aspect=.9);
plt.title('Vehicle Age')

"""- We can observe that as the age of the vehicle increases, specifically more than 2 years, people tend to be less interested in purchasing Health Insurance.

### 2.1.8 Vehicle Damage
"""

train['Vehicle_Damage'].value_counts()

sns.catplot(x = 'Vehicle_Damage', data= train, kind ='count')
plt.title('Vehicle Damage Distribution')

"""- The ratio look the same, have no special different

### 2.1.9 Annual_Premium
"""

train['Annual_Premium'].describe()

plt.figure(figsize=(10, 6))
sns.histplot(train['Annual_Premium'], bins=50, kde=True)
plt.title('Distribution of Annual Premium')
plt.xlabel('Annual Premium')
plt.ylabel('Frequency')
plt.show()

plt.figure(figsize=(10, 6))
sns.boxplot(train['Annual_Premium'])
plt.title('Boxplot of Annual Premium')
plt.xlabel('Annual Premium')
plt.show()

"""- There are outliers present, but they should not be removed because these outliers reflect real-world information and represent a specific group of consumers.
- Furthermore, the annual insurance expenditure is still relatively low.

### 2.1.10 Policy_Sales_Channel
"""

train['Policy_Sales_Channel'].value_counts()

plt.figure(figsize=(40, 10))
sns.countplot(x='Policy_Sales_Channel', data=train)
plt.title('Distribution of Policy Sales Channel')
plt.xlabel('Policy Sales Channel')
plt.ylabel('Count')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(12, 6))
sns.violinplot(x='Policy_Sales_Channel', data=train)
plt.title('Violin Plot of Policy Sales Channel')
plt.xlabel('Policy Sales Channel')
plt.xticks(rotation=90)
plt.show()

plt.figure(figsize=(40,10))
train['Policy_Sales_Channel'].value_counts().plot.bar()
plt.title("Policy_Sales_Channel")

"""- Policy_Sales_Channel no. 152 have higest number of customers.
- Policy_Sales_Channel no. [152,26,124,160,156,122,157,154,151,163] have most of the customers.

### 2.1.11 Vintage
"""

train['Vintage'].value_counts()

plt.figure(figsize= (40,10))
sns.distplot(train['Vintage'], bins = 40)
plt.title('Distribution of Vintage')

"""- From above we can also depict that Vintage has a approximatly uniform distribution.

### 2.1.12 Response
"""

train['Response'].value_counts()

sns.countplot(x = 'Response', data = train)

"""- Rõ ràng nhìn thấy được rằng số người muốn mua BHYT thấp hơn hẳn, cụ thể chỉ có 46710 người, chiếm 12,25%.

## 2.2 Multivariate Charts:

### 2.2.1 Finding Outliers
"""

def show_outliers(df):
    fig, axes = plt.subplots(1, 3, figsize=(22,12))

    # Box plots for categorical comparison by Response
    sns.boxplot(ax=axes[0], y='Annual_Premium', x='Response', data=df)
    axes[0].set_xlabel('Response', fontsize=14)
    axes[0].set_ylabel('Annual_Premium', fontsize=14)
    axes[0].set_title('Annual_Premium', fontsize=15, fontweight='bold')

    sns.boxplot(ax=axes[1], y='Age', x='Response', data=df)
    axes[1].set_xlabel('Response', fontsize=14)
    axes[1].set_ylabel('Age', fontsize=14)
    axes[1].set_title('Age', fontsize=15, fontweight='bold')

    sns.boxplot(ax=axes[2], y='Vintage', x='Response', data=df)
    axes[2].set_xlabel('Response', fontsize=14)
    axes[2].set_ylabel('Vintage', fontsize=14)
    axes[2].set_title('Vintage', fontsize=15, fontweight='bold')

# Execute the corrected function with the data DataFrame
show_outliers(train)

"""**Box Plots for Categorical Comparison by Response**
- Annual Premium: The box plot shows a wide range of annual premiums for both responses (interested = 1 and not interested = 0), with a significant number of outliers indicating extremely high premiums compared to the majority. This suggests that while most customers pay within a typical range, a subset faces much higher premiums.
- Age: The age distribution relative to the response indicates that older individuals may show more interest in insurance products, as seen by potentially higher median ages for those interested (Response = 1). Age columns has some outliers but we are not going to treat them because it won't be affecting our result.
- Vintage: The number of days customers have been insured shows no clear pattern of outliers significantly influenced by the response. The distribution seems fairly consistent across both categories, indicating that the length of association with the insurer does not significantly affect interest in insurance products.

### 2.2.2 Outlier Treatment and Feature Scaling
"""

def outlier_treatment(df):
    Q1=df['Annual_Premium'].quantile(0.25)
    Q3=df['Annual_Premium'].quantile(0.75)
    IQR=Q3-Q1

    Lower_Whisker = Q1-1.5*IQR
    Upper_Whisker = Q3+1.5*IQR
    df['Annual_Premium_Treated'] = np.where(df['Annual_Premium']>Upper_Whisker, Upper_Whisker, df['Annual_Premium'])

def scale_features(df):
    scaler = MinMaxScaler()

    df['Annual_Premium_Treated'] = scaler.fit_transform(df['Annual_Premium_Treated'].values.reshape(-1,1))
    df['Vintage_Treated'] = scaler.fit_transform(df['Vintage'].values.reshape(-1,1))

outlier_treatment(train)
scale_features(train)

def show_ann_prem_outliers(df):

    fig, axes = plt.subplots(1, 2, figsize=(15,5))
    sns.boxplot(ax = axes[0], y = 'Annual_Premium_Treated',x = 'Response', data = df)
    axes[0].set_xlabel(xlabel = 'Response', fontdict={'fontsize': 14})
    axes[0].set_ylabel(ylabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[0].set_title('Annual Premium Treated', fontdict={'fontsize': 15,  'fontweight' :'bold'})

    sns.distplot(ax = axes[1], x = df['Annual_Premium_Treated'], color='brown')
    axes[1].set_xlabel(xlabel = 'Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[1].set_ylabel(ylabel = 'Density', fontdict={'fontsize': 14})
    axes[1].set_title('Annual Premium Treated', fontdict={'fontsize': 15,  'fontweight' :'bold'})


show_ann_prem_outliers(train)

"""- From the above plots we can see that there are no more outliers in Annual Premium.

### 2.2.3 Exploring the Numerical Features

We have 4 numerical features: Age, Policy_Sales_Channel, Region_Code, Vintage and Annual_Prenium. But the Annual_Prenium have been deal earlier so we can pass it. Without any further delay, let's explore these left features.
"""

def show_distribution_numerical_features(df):
    fig, axes = plt.subplots(2,2, figsize=(20,15))

    sns.countplot(ax = axes[0][0],x = 'Age', data = df, hue='Response')
    axes[0][0].set_xlabel(xlabel = 'Age', fontdict={'fontsize': 14})
    axes[0][0].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})
    axes[0][0].set_title('Age', fontdict={'fontsize': 15,  'fontweight' :'bold'})
    axes[0][0].set_xticklabels(axes[0][0].get_xticklabels(), rotation=90)

    sns.countplot(ax = axes[0][1],x = 'Region_Code', data = df, hue='Response')
    axes[0][1].set_xlabel(xlabel = 'Region_Code', fontdict={'fontsize': 14})
    axes[0][1].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})
    axes[0][1].set_title('Region_Code',fontdict={'fontsize': 15,  'fontweight' :'bold'})
    axes[0][1].set_xticklabels(axes[0][1].get_xticklabels(), rotation=90)

    sns.countplot(ax = axes[1][0],x = 'Policy_Sales_Channel', data = df, hue='Response')
    axes[1][0].set_xlabel(xlabel = 'Policy_Sales_Channel', fontdict={'fontsize': 14})
    axes[1][0].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})
    axes[1][0].set_title('Policy_Sales_Channel',fontdict={'fontsize': 15,  'fontweight' :'bold'})
    axes[1][0].set_xticklabels(axes[1][0].get_xticklabels(), rotation=90)

    sns.histplot(ax = axes[1][1], x = train['Vintage'],data = df, hue='Response')
    axes[1][1].set_xlabel(xlabel = 'Vintage', fontdict={'fontsize': 14})
    axes[1][1].set_ylabel(ylabel = 'Count', fontdict={'fontsize': 14})
    axes[1][1].set_title('Vintage',fontdict={'fontsize': 15,  'fontweight' :'bold'})
    axes[1][1].set_xticklabels(axes[1][1].get_xticklabels(), rotation=90)

    plt.suptitle('Distribution of Numerical Features', fontsize = 22, fontweight = 'bold' )

show_distribution_numerical_features(train)

"""1. Age:
- The interest in vehicle insurance (indicated by the Response variable) seems to be higher in the age group that is slightly older than the youngest group.

2. Region_Code:
- Certain regions (e.g., code around 28) have a significantly higher count of customers than others.
- While there's an overall low interest in vehicle insurance across most regions, the regions with more customers also show more customers interested in vehicle insurance.

3. Policy_Sales_Channel:
- A few channels are particularly dominant in terms of count, specifically those with anonymized codes near the lower end of the range.
- There's a visible difference in the number of positive responses across different sales channels. Some channels seem to be more effective in garnering interest in vehicle insurance.

4. Vintage:
- The distribution of customers across the number of days they have been associated with the company seems quite uniform.
- There is no clear trend that suggests the length of association with the company (Vintage) has a strong influence on the interest in vehicle insurance.

5. General Insights:
- There is a class imbalance visible in all the features with respect to the Response variable. The number of customers not interested in vehicle insurance (Response 0) significantly outweighs those who are interested (Response 1).
**It's important to consider this imbalance when building a predictive model, as it may influence the performance and the evaluation metrics chosen for the model.**
"""

def show_violin_distribution(df):

    sns.catplot(y = 'Age', data = df, x='Response', kind = 'violin')
    plt.xlabel(xlabel = 'Response', fontdict={'fontsize': 14})
    plt.ylabel(ylabel = 'Age', fontdict={'fontsize': 14})
    plt.title('Age Distribution', fontdict={'fontsize': 20, 'fontweight':'bold'})

    sns.catplot(y = 'Region_Code', data = df, x='Response', kind = 'violin')
    plt.xlabel(xlabel = 'Response', fontdict={'fontsize': 14})
    plt.ylabel(ylabel = 'Region_Code', fontdict={'fontsize': 14})
    plt.title('Region Code Distribution', fontdict={'fontsize': 20, 'fontweight':'bold'})

    sns.catplot(y = 'Policy_Sales_Channel', data = df, x='Response', kind = 'violin')
    plt.xlabel(xlabel = 'Response', fontdict={'fontsize': 14})
    plt.ylabel(ylabel = 'Policy_Sales_Channel', fontdict={'fontsize': 14})
    plt.title('Policy Sales Channel Distribution', fontdict={'fontsize': 20, 'fontweight':'bold'})

show_violin_distribution(train)

"""**From the above graphical representation we can conclude on a few points:**
- As we can see, we have a huge dispersion of data in Age feature, so in order to gain better insights on *Age* feature, we can convert it into categories as YoungAge, MiddleAge and OldAge. Young Age: 20<= x <=34. Middle Age: 34-> 60. Old Age: Else
- Similarly, we can also categorize *Region Code* and *Policy_Sales_Channel*.
**Làm report đọc code dưới để biết khoảng**

### 2.2.4 Converting Numerical Columns to Categorical
"""

def convert_numerical_to_categorical(df):
    # Categorizing Age feature
    df['Age_Group'] = df['Age'].apply(lambda x:'YoungAge' if x >= 20 and x<=34 else 'MiddleAge' if x>34 and x<=61 else 'OldAge')

    # Categorizing Policy_Sales_Channel feature
    x = df['Policy_Sales_Channel'].value_counts().apply(lambda x: 'Channel_A' if x>100000 else 'Channel_B' if 74000<x<100000 else 'Channel_C' if 10000<x<=74000 else 'Channel_D')
    res = dict(zip(x.keys(),x.values))
    df['Policy_Sales_Channel_Categorical'] = df['Policy_Sales_Channel'].map(res)

    # Categorizing Region Code feature
    region_counts = train['Region_Code'].value_counts()
    region_categories = region_counts.apply(
        lambda x: 'Region_A' if x >= 100000 else 'Region_B' if 11000 < x < 100000 else 'Region_C'
    )
    region_mapping = region_categories.to_dict()
    train['Region_Code_Categorical'] = train['Region_Code'].map(region_mapping)

convert_numerical_to_categorical(train)

def show_distribution_num_to_cat(train):
    fig, axes = plt.subplots(1, 3, figsize=(22, 5))

    age_order = ['YoungAge', 'MiddleAge', 'OldAge']
    sns.countplot(ax=axes[0], x='Age_Group', data=train, hue='Response', order=age_order)
    axes[0].set_xlabel(xlabel='Age_Group', fontdict={'fontsize': 14})
    axes[0].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
    axes[0].set_title('Age', fontdict={'fontsize': 15})

    region_order = ['Region_A', 'Region_B', 'Region_C']
    sns.countplot(ax=axes[1], x='Region_Code_Categorical', data=train, hue='Response', order=region_order)
    axes[1].set_xlabel(xlabel='Region_Code_Categorical', fontdict={'fontsize': 14})
    axes[1].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
    axes[1].set_title('Region_Code', fontdict={'fontsize': 15})

    channel_order = ['Channel_A', 'Channel_B', 'Channel_C', 'Channel_D']
    sns.countplot(ax=axes[2], x='Policy_Sales_Channel_Categorical', data=train, hue='Response', order=channel_order)
    axes[2].set_xlabel(xlabel='Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})
    axes[2].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
    axes[2].set_title('Policy_Sales_Channel', fontdict={'fontsize': 15})

    plt.suptitle('Distribution of Categorical Features', fontsize=22, fontweight='bold')
show_distribution_num_to_cat(train)

"""**Observations:**
*   We can see that Customers belonging to *YoungAge* group are more likely not interested in taking the vehicle insurance.
*   Similarly, *Region_C* and *Channel_A* Customers has the highest chances of not taking the vehicle insurance.

### 2.2.5 Exploring Gender Correlations
"""

def show_gender_response_relation(df):
    sns.catplot(x="Response", hue="Gender", kind="count",
                palette="pastel",
                data=df)
    plt.xlabel('Response', fontdict={'fontsize':12})
    plt.ylabel('Count',fontdict={'fontsize': 14})
    plt.title('Response V/S Gender', fontdict={'fontsize': 15, 'fontweight':'bold'})

show_gender_response_relation(train)

"""- For the above plot, we can say that the no. of male customers in our data set is higher than female customers.

### 2.2.1 Exploring Age Correlations
"""

def show_age_correlation(train):

    fig, axes = plt.subplots(2, 3, figsize=(22, 15))

    sns.boxplot(ax=axes[0, 0], x="Response", y="Age", data=train)
    axes[0, 0].set_xlabel(xlabel='Response (0 = No, 1 = Yes)', fontdict={'fontsize': 14})
    axes[0, 0].set_ylabel(ylabel='Age', fontdict={'fontsize': 14})
    axes[0, 0].set_title('Figure 1: Age and Response Correlation', fontdict={'fontsize': 15, 'fontweight':'bold'})

    sns.boxplot(ax=axes[0, 1], x="Driving_License", y="Age", data=train)
    axes[0, 1].set_xlabel(xlabel='Driving_License (0 = No, 1 = Yes)', fontdict={'fontsize': 14})
    axes[0, 1].set_ylabel(ylabel='Age', fontdict={'fontsize': 14})
    axes[0, 1].set_title('Figure 2: Age and Driving_License Correlation', fontdict={'fontsize': 15, 'fontweight':'bold'})

    sns.boxplot(ax=axes[0, 2], x="Previously_Insured", y="Age", data=train)
    axes[0, 2].set_xlabel(xlabel='Previously_Insured (0 = No, 1 = Yes)', fontdict={'fontsize': 14})
    axes[0, 2].set_ylabel(ylabel='Age', fontdict={'fontsize': 14})
    axes[0, 2].set_title('Figure 3: Age and Previously_Insured Correlation', fontdict={'fontsize': 15, 'fontweight':'bold'})

    sns.boxplot(ax=axes[1, 0], x="Vehicle_Age", y="Age", data=train)
    axes[1, 0].set_xlabel(xlabel='Vehicle_Age', fontdict={'fontsize': 14})
    axes[1, 0].set_ylabel(ylabel='Age', fontdict={'fontsize': 14})
    axes[1, 0].set_title('Figure 4: Age and Vehicle_Age Correlation', fontdict={'fontsize': 15, 'fontweight':'bold'})

    sns.scatterplot(ax=axes[1, 1], x=train['Age'], y=train['Annual_Premium'])
    axes[1, 1].set_xlabel(xlabel='Age', fontdict={'fontsize': 14})
    axes[1, 1].set_ylabel(ylabel='Annual_Premium', fontdict={'fontsize': 14})
    axes[1, 1].set_title('Figure 5: Age Vs Annual Premium', fontdict={'fontsize': 15, 'fontweight':'bold'})

    plt.suptitle('Exploring Age Correlations', fontsize=30, fontweight='bold')

# Assuming 'train' is the dataframe containing the required columns
show_age_correlation(train)

"""- There is a paradox here, specifically in Figure 1, where it is easily noticeable that young individuals who have purchased Health Insurance tend to show little interest in Vehicle Insurance. Instead, those in the 40-50 age group who have acquired Health Insurance seem to be more concerned about Vehicle Insurance.

- But in Figure 3, the 'Previously_Insured' variable (1: Customer already has Vehicle Insurance, 0: Customer doesn't have Vehicle Insurance) indicates that the young age group has already purchased Vehicle Insurance.

**Therefore, it can be concluded that individuals who respond with '1' for 'Previously_Insured' tend to have a lower likelihood of expressing a need to purchase or switch their Vehicle Insurance coverage.**
"""

def show_age_relations(train):
    fig, axes = plt.subplots(1, 3, figsize=(25, 8))
    sns.countplot(ax=axes[0], x="Response", hue="Age_Group", palette="pastel", data=train)
    axes[0].set_xlabel(xlabel='Response', fontdict={'fontsize': 14})
    axes[0].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
    axes[0].set_title('Age_Group', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.histplot(ax=axes[1], binwidth=0.5, x="Age_Group", hue="Previously_Insured", data=train, stat="count", multiple="stack")
    axes[1].set_xlabel(xlabel='Age_Group', fontdict={'fontsize': 14})
    axes[1].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
    axes[1].set_title('Age_Group V/S Previously_Insured', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.lineplot(ax=axes[2], x="Age", y="Annual_Premium_Treated", data=train, hue="Gender")
    axes[2].set_xlabel(xlabel='Age', fontdict={'fontsize': 14})
    axes[2].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[2].set_title('Age V/S Annual Premium Treated', fontdict={'fontsize': 15, 'fontweight': 'bold'})

show_age_relations(train)

"""Based on the visualizations in the image you've uploaded, here are the insights:

1. **Age_Group vs Response**:
   - The countplot shows that the largest group of customers who are not interested in vehicle insurance (`Response` 0) are in the MiddleAge group, followed by YoungAge and then OldAge.
   - Conversely, for customers who are interested in vehicle insurance (`Response` 1), the MiddleAge group also shows a higher count compared to the other age groups, but the difference between age groups is less pronounced compared to those not interested.

2. **Age_Group vs Previously_Insured**:
   - The histogram suggests a significant number of MiddleAge customers who are not previously insured are present in the dataset, possibly indicating a potential market for selling vehicle insurance.
   - For the OldAge and YoungAge groups, there's a more balanced distribution between those who are previously insured and those who are not.

3. **Age vs Annual Premium Treated (with Gender differentiation)**:
   - The line plot indicates that the annual premium treated (which seems to be some form of normalized or adjusted premium) generally increases with age for both genders.
   - There's a convergence of premium amounts between genders as age increases, particularly noticeable from around age 50 onwards.
   - There are some fluctuations and peaks in premium amounts at certain ages, but the general trend is upwards with age.
   - Notably, after a certain age (around 70), the trend shows a sharp decline for both genders, which could be due to various factors such as reduced insurance coverage or lower risk assessment for the elderly.

These insights could be valuable for the insurance company to tailor their marketing and product strategies, focusing on the MiddleAge group who show interest in vehicle insurance and are not previously insured. Additionally, understanding the relationship between age and premium cost could assist in developing age-sensitive pricing strategies.

### 2.2.6 Exploring Vehicle Damage
"""

fig = px.pie(train, values='Response', names='Vehicle_Damage', title='Vehicle Damage Distribution')
fig.show()

def show_vechile_damage_relations(train):
    fig, axes = plt.subplots(1, 2, figsize=(22, 8))

    sns.pointplot(ax=axes[0], x="Vehicle_Damage", y="Response", hue="Vehicle_Age", data=train)
    axes[0].set_xlabel(xlabel='Vehicle_Damage', fontdict={'fontsize': 14})
    axes[0].set_ylabel(ylabel='Response', fontdict={'fontsize': 14})
    axes[0].set_title('Vehicle_Damage V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.pointplot(ax=axes[1], x='Vehicle_Damage', y='Annual_Premium_Treated', data=train)
    axes[1].set_xlabel(xlabel='Vehicle_Damage', fontdict={'fontsize': 14})
    axes[1].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[1].set_title('Vehicle_Damage V/S Annual_Premium_Treated', fontdict={'fontsize': 15, 'fontweight': 'bold'})

show_vechile_damage_relations(train)

"""**Observations:**
- *Pie* plot shows the number of customers whose vehicle are damaged/not damaged and they took the insurance.
- From the first *point* plot, we can say that the chances of taking a vehicle insurance is higher if vehicle is damaged irrespective of *VehicleAge* group. With the increase in vehicle age, the chances of taking vehicle insurance also increases.
- The second point plot says that the *Annual_Premium* is comparetively higher for customers with damaged vehicle.

The visualizations present the relationship between vehicle damage, vehicle age, and customer responses or interest in insurance, as well as the correlation with annual premium costs:

1. **Vehicle Damage vs Response**: Customers with vehicle damage are more likely to show interest in vehicle insurance across all vehicle ages. The interest is highest among those with vehicles older than 2 years.

2. **Vehicle Damage vs Annual Premium Treated**: Customers who have experienced vehicle damage seem to pay a slightly higher annual premium than those who have not. However, there's a notable variance in the premium for those with no vehicle damage, suggesting differing premium calculations based on other factors.

### 2.2.7 Exploring Vehicle Age
"""

plt.figure(figsize=(10, 8))
order_Vehicle_Age = ["< 1 Year", "1-2 Year", "> 2 Years"]
sns.countplot(x='Vehicle_Age', hue='Response', data=train, palette="Dark2", order = order_Vehicle_Age )
plt.xlabel(xlabel='Vehicle_Age', fontdict={'fontsize': 14})
plt.ylabel(ylabel='Count', fontdict={'fontsize': 14})
plt.title('Vehicle_Age', fontdict={'fontsize': 15, 'fontweight': 'bold'})

def show_vehicle_age_relation(train):
    fig, axes = plt.subplots(2, 3, figsize=(22, 15))

    year_order = ["< 1 Year", "1-2 Year", "> 2 Years"]

    sns.barplot(ax=axes[0][0], x='Vehicle_Age', y='Response', data=train, order = year_order)
    axes[0][0].set_xlabel(xlabel='Vehicle_Age', fontdict={'fontsize': 14})
    axes[0][0].set_ylabel(ylabel='Response', fontdict={'fontsize': 14})
    axes[0][0].set_title('Vehicle_Age V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.pointplot(ax=axes[0][1], y='Response', x='Vehicle_Age', hue='Vehicle_Damage', data=train, order = year_order)
    axes[0][1].set_xlabel(xlabel='Vehicle_Age', fontdict={'fontsize': 14})
    axes[0][1].set_ylabel(ylabel='Response', fontdict={'fontsize': 14})
    axes[0][1].set_title('Vehicle_Age V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})
    axes[0][1].legend(loc='upper right')

    sns.pointplot(ax=axes[0][2], y='Response', x='Vehicle_Age', hue='Region_Code_Categorical', data=train, order = year_order)
    axes[0][2].set_xlabel(xlabel='Vehicle_Age', fontdict={'fontsize': 14})
    axes[0][2].set_ylabel(ylabel='Response', fontdict={'fontsize': 14})
    axes[0][2].set_title('Vehicle_Age V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})
    axes[0][2].legend(loc='upper right')

    sns.pointplot(ax=axes[1][0], y='Response', x='Vehicle_Age', hue='Policy_Sales_Channel_Categorical', data=train, order = year_order)
    axes[1][0].set_xlabel(xlabel='Vehicle_Age', fontdict={'fontsize': 14})
    axes[1][0].set_ylabel(ylabel='Response', fontdict={'fontsize': 14})
    axes[1][0].set_title('Vehicle_Age V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})
    axes[1][0].legend(loc='upper right')

    sns.boxplot(ax=axes[1][1], y='Annual_Premium_Treated', x='Vehicle_Age', hue='Vehicle_Damage', data=train, order = year_order)
    axes[1][1].set_xlabel(xlabel='Vehicle_Age', fontdict={'fontsize': 14})
    axes[1][1].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[1][1].set_title('Vehicle_Age V/S Annual_Premium_Treated', fontdict={'fontsize': 15, 'fontweight': 'bold'})
    axes[1][1].legend(loc='upper right')

    sns.stripplot(ax=axes[1][2], y='Annual_Premium_Treated', x='Vehicle_Age', hue='Vehicle_Damage', data=train, order = year_order)
    axes[1][2].set_xlabel(xlabel='Vehicle_Age', fontdict={'fontsize': 14})
    axes[1][2].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[1][2].set_title('Vehicle_Age V/S Annual_Premium_Treated', fontdict={'fontsize': 15, 'fontweight': 'bold'})
    axes[1][2].legend(loc='upper right')

    plt.show()

show_vehicle_age_relation(train)

"""The charts provide insights into how vehicle age relates to customer response to insurance offerings, and how it correlates with the annual premium:

1. **Vehicle Age vs Response**: There is a clear trend showing that as the vehicle age increases, the likelihood of a customer being interested in vehicle insurance also increases. Customers with vehicles older than 2 years have the highest response rate, suggesting they may perceive a greater need for insurance.

2. **Vehicle Age by Policy Sales Channel**: Different sales channels appear to be more effective with certain vehicle age groups. For instance, Channel_D seems to be most effective with customers having vehicles older than 2 years.

3. **Vehicle Age vs Annual Premium Treated**: There's a wide distribution of treated annual premiums across all vehicle ages, indicating that age alone does not determine premium costs. However, older vehicles (> 2 Years) seem to have a higher median annual premium, which could be due to higher perceived risks or possible correlations with other factors such as vehicle make, model, or repair history.

4. **Regional Analysis**: Customer interest also varies by region, with some regions showing a much higher likelihood of being interested in insurance as vehicle age increases.

These insights suggest that targeting strategies for vehicle insurance could benefit from considering both the age of the vehicle and the region, as well as focusing on sales channels that are more effective for certain customer segments.

### 2.2.8 Exploring Annual Premium
"""

def show_annual_premium_relation(train):
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    sns.pointplot(ax=axes[0][0], x='Response', y='Annual_Premium_Treated', data=train)
    axes[0][0].set_xlabel(xlabel='Response', fontdict={'fontsize': 14})
    axes[0][0].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[0][0].set_title('Annual_Premium_Treated V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.violinplot(ax=axes[0][1], x='Response', y='Annual_Premium_Treated', data=train)
    axes[0][1].set_xlabel(xlabel='Response', fontdict={'fontsize': 14})
    axes[0][1].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[0][1].set_title('Annual_Premium_Treated V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.stripplot(ax=axes[1][0], x='Response', y='Annual_Premium_Treated', data=train[:1000], jitter=True)
    axes[1][0].set_xlabel(xlabel='Response', fontdict={'fontsize': 14})
    axes[1][0].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[1][0].set_title('Annual_Premium_Treated V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.stripplot(ax=axes[1][1], x='Response', y='Annual_Premium_Treated', data=train, jitter=True)
    axes[1][1].set_xlabel(xlabel='Response', fontdict={'fontsize': 14})
    axes[1][1].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[1][1].set_title('Annual_Premium_Treated V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})

show_annual_premium_relation(train)

"""The visuals indicate that there is a relationship between the annual premium (treated) and the customer's response to vehicle insurance:

1. Customers who showed interest in vehicle insurance (Response 1) tend to have a higher median annual premium than those who did not show interest (Response 0). This suggests that customers willing to pay a higher premium may also be more interested in purchasing insurance.

2. The distribution plots reveal that while there is a broad range of premiums paid by both groups, the variation in premiums is similar across customers who are interested and those who are not. This points to the fact that premium levels alone do not fully explain the likelihood of a customer being interested in insurance.

3. Despite the higher median annual premium for interested customers, the overlap in the range of premiums paid by both interested and uninterested customers implies that factors other than the annual premium also play a significant role in the decision to purchase insurance.

In conclusion, while annual premium levels are higher on average for those interested in vehicle insurance, the impact of the premium on a customer's response is not absolute and should be considered alongside other variables.

### 2.2.9 Exploring Annual Premium and Age
"""

def show_annual_premium_with_age_group(train):
    fig, axes = plt.subplots(1, 2, figsize=(15, 8))

    age_order = ['YoungAge', 'MiddleAge', 'OldAge']
    sns.barplot(ax=axes[0], y='Annual_Premium_Treated', x='Age_Group', data=train, order = age_order)
    axes[0].set_xlabel(xlabel='Age_Group', fontdict={'fontsize': 14})
    axes[0].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[0].set_title('Annual_Premium_Treated V/S Age_Group', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.violinplot(ax=axes[1], y='Annual_Premium_Treated', x='Age_Group', data=train, order = age_order)
    axes[1].set_xlabel(xlabel='Age_Group', fontdict={'fontsize': 14})
    axes[1].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[1].set_title('Annual_Premium_Treated V/S Age_Group', fontdict={'fontsize': 15, 'fontweight': 'bold'})

show_annual_premium_with_age_group(train)

def show_age_annual_premium_relation(train):
    plt.figure(figsize=(14, 9))
    plt.hexbin(data=train, x='Age', y='Annual_Premium_Treated', gridsize=30, cmap='Greens')
    plt.title('Annual Premium V/S Age', fontdict={'fontsize': 15, 'fontweight': 'bold'})
    plt.ylabel('Annual Premium Treated', fontdict={'fontsize': 14})
    plt.xlabel('Age', fontdict={'fontsize': 14})
    plt.show()

    import plotly.express as px
    fig = px.scatter(train, x="Age", y="Annual_Premium_Treated",
                    color="Region_Code_Categorical",
                    size_max=180, opacity=0.3, title='Age V/S Annual Premium')
    fig.show()

show_age_annual_premium_relation(train)

"""### 2.2.10 Exploring Age Categorical"""

def age_group_distribution(train):
    fig, axes = plt.subplots(1, 3, figsize=(15, 6))

    colors = sns.color_palette('pastel')[0:4]
    explode = (0.01, 0.25, 0.01)
    axes[0].pie(x=train.groupby('Age_Group')['Response'].sum(), explode=explode,
                labels=train['Age_Group'].unique(), colors=colors, autopct='%1.1f%%',
                shadow=True)
    axes[0].set_title('with Response', fontsize=15, fontweight='bold', pad=15)

    axes[1].pie(x=train.groupby('Age_Group')['Annual_Premium'].sum(), explode=explode,
                labels=train['Age_Group'].unique(), colors=colors, autopct='%1.1f%%',
                shadow=True)
    axes[1].set_title('with Annual_Premium', fontsize=15, fontweight='bold', pad=15)

    axes[2].pie(x=train.groupby('Age_Group')['Previously_Insured'].sum(), explode=explode,
                labels=train['Age_Group'].unique(), colors=colors, autopct='%1.1f%%',
                shadow=True)
    axes[2].set_title('with Previously_Insured', fontsize=15, fontweight='bold', pad=15)

    plt.suptitle('Age Group Distribution', fontsize=20, fontweight='bold')

age_group_distribution(train)

def show_region_code_distribution(df):

    colors = sns.color_palette('pastel')[0:4]
    explode = (0.01, 0.01, 0.01)

    fig, axes = plt.subplots(1,2, figsize=(15,6))
    axes[0].pie(x=df.groupby('Region_Code_Categorical')['Vintage'].sum(),explode=explode,
                labels=train['Region_Code_Categorical'].unique(), colors=colors,autopct='%1.1f%%',
                shadow=True);
    axes[0].set_title('with Vintage', fontsize = 15, fontweight ='bold', pad=15)

    axes[1].pie(x=df.groupby('Region_Code_Categorical')['Annual_Premium_Treated'].sum(),explode=explode,
                labels=train['Region_Code_Categorical'].unique(), colors=colors, autopct='%1.1f%%',
                shadow=True);
    axes[1].set_title('with Annual_Premium', fontsize = 15, fontweight ='bold', pad=15)

    plt.suptitle('Region Code Distribution',fontsize = 15, fontweight ='bold')

show_region_code_distribution(train)

"""### 2.2.11 Exploring Policy Sales Channel"""

def show_policy_sales_channel_relation(train):

    fig, axes = plt.subplots(2, 3, figsize=(22, 15))
    channel_order = ['Channel_A', 'Channel_B', 'Channel_C', 'Channel_D']

    sns.pointplot(ax=axes[0][0], x='Policy_Sales_Channel_Categorical', y='Vintage', data=train)
    axes[0][0].set_xlabel(xlabel='Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})
    axes[0][0].set_ylabel(ylabel='Vintage', fontdict={'fontsize': 14})
    axes[0][0].set_title('Policy_Sales_Channel V/S Vintage', fontdict={'fontsize': 15, 'fontweight':'bold'})

    sns.pointplot(ax=axes[0][1], x='Policy_Sales_Channel_Categorical', y='Annual_Premium_Treated', data=train)
    axes[0][1].set_xlabel(xlabel='Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})
    axes[0][1].set_ylabel(ylabel='Annual_Premium_Treated', fontdict={'fontsize': 14})
    axes[0][1].set_title('Policy_Sales_Channel V/S Annual_Premium_Treated', fontdict={'fontsize': 15, 'fontweight':'bold'})

    train['Policy_Sales_Channel_Categorical'].value_counts().plot(ax=axes[0][2], kind='barh')
    axes[0][2].set_xlabel(xlabel='Count', fontdict={'fontsize': 14})
    axes[0][2].set_ylabel(ylabel='Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})
    axes[0][2].set_title('Policy_Sales_Channel', fontdict={'fontsize': 15, 'fontweight':'bold'})

    sns.histplot(ax=axes[1][0], x="Policy_Sales_Channel_Categorical", hue="Response", data=train, stat="count",
                 multiple="stack", binwidth=0.5)
    axes[1][0].set_xlabel(xlabel='Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})
    axes[1][0].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
    axes[1][0].set_title('Policy_Sales_Channel', fontdict={'fontsize': 15, 'fontweight':'bold'})

    groupPolicySalesBySum = train.groupby(by=["Policy_Sales_Channel_Categorical"])[['Response']].sum().reset_index()
    sns.barplot(ax=axes[1][1], x="Policy_Sales_Channel_Categorical", y="Response", data=groupPolicySalesBySum)
    axes[1][1].set_xlabel(xlabel='Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})
    axes[1][1].set_ylabel(ylabel='Response', fontdict={'fontsize': 14})
    axes[1][1].set_title('Policy_Sales_Channel V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})

    sns.barplot(ax=axes[1][2], x='Policy_Sales_Channel_Categorical', y='Response', data=train, hue='Region_Code_Categorical')
    axes[1][2].set_xlabel(xlabel='Policy_Sales_Channel_Categorical', fontdict={'fontsize': 14})
    axes[1][2].set_ylabel(ylabel='Response', fontdict={'fontsize': 14})
    axes[1][2].set_title('Policy_Sales_Channel V/S Response', fontdict={'fontsize': 15, 'fontweight':'bold'})

    plt.suptitle('Exploring Policy Sales Channel', fontsize=15, fontweight='bold')

show_policy_sales_channel_relation(train)

"""### 2.2.12 Exploring Driving License"""

def show_Driving_License_relation(train):
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))

    age_order = ['YoungAge', 'MiddleAge', 'OldAge']

    sns.pointplot(ax=axes[0][0], x='Response', y='Driving_License', data=train)
    axes[0][0].set_xlabel(xlabel='Response', fontdict={'fontsize': 14})
    axes[0][0].set_ylabel(ylabel='Driving_License', fontdict={'fontsize': 14})
    axes[0][0].set_title('Driving_License V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.violinplot(ax=axes[0][1], x='Response', y='Driving_License', data=train)
    axes[0][1].set_xlabel(xlabel='Response', fontdict={'fontsize': 14})
    axes[0][1].set_ylabel(ylabel='Driving_License', fontdict={'fontsize': 14})
    axes[0][1].set_title('Driving_License V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    sns.histplot(ax=axes[1][0], binwidth=0.5, x="Response", hue="Driving_License", data=train, stat="count", multiple="stack")
    axes[1][0].set_xlabel(xlabel='Response', fontdict={'fontsize': 14})
    axes[1][0].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
    axes[1][0].set_title('Driving_License V/S Response', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    age_cat = pd.Categorical(train['Age_Group'], categories=age_order, ordered=True)
    sns.histplot(ax=axes[1][1], binwidth=0.5, x=age_cat, hue="Driving_License", data=train, stat="count", multiple="stack")
    axes[1][1].set_xlabel(xlabel='Age_Group', fontdict={'fontsize': 14})
    axes[1][1].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
    axes[1][1].set_title('Driving_License V/S Age_Group', fontdict={'fontsize': 15, 'fontweight': 'bold'})

    ax_hist_age_license = axes[1, 1].twinx()

    plt.show()

show_Driving_License_relation(train)

"""### 2.2.13 Exploring Previously_Insured Correlations

As discussed earlier, we can observe that individuals who have already purchased Vehicle Insurance still include some who express an interest in Vehicle Insurance. This suggests that there is a subgroup of customers who may either be **considering changing their insurance plans** or **providing inaccurate responses**.
"""

# Filter the data for those with Previously_Insured = 1 and Response = 1
filtered_data = train[(train['Previously_Insured'] == 1) & (train['Response'] == 1)]

# Count the number of occurrences
count_data = filtered_data.groupby(['Previously_Insured', 'Response']).size().reset_index(name='Count')

# Print the Result
result_count = count_data['Count'].values[0]
print(f"Number of customers who have already purchased Vehicle Insurance but still showing interest in Vehicle Insurance: {result_count} customers")

# Set up the matplotlib figure for subplots
fig, axes = plt.subplots(2, 2, figsize=(18, 12))

# Relationship between Previously_Insured, Age, and Response
sns.boxplot(ax=axes[0, 0], x='Previously_Insured', y='Age', hue='Response', data=train)
axes[0, 0].set_title('Age Distribution by Previously Insured Status and Response')

# Relationship between Previously_Insured, Annual_Premium, and Response
sns.boxplot(ax=axes[0, 1], x='Previously_Insured', y='Annual_Premium', hue='Response', data=train)
axes[0, 1].set_title('Annual Premium Distribution by Previously Insured Status and Response')

# Relationship between Previously_Insured, Driving_License, and Response
sns.countplot(ax=axes[1, 0], x='Previously_Insured', hue='Response', data=train[train['Driving_License']==1])
axes[1, 0].set_title('Response Distribution for Previously Insured Customers with Driving License')

# Relationship between Previously_Insured, Vehicle_Damage, and Response
sns.countplot(ax=axes[1, 1], x='Previously_Insured', hue='Response', data=train, palette="Set2")
axes[1, 1].set_title('Response Distribution for Previously Insured Customers by Vehicle Damage')

plt.tight_layout()
plt.show()

"""The visualizations show the relationship between 'Previously_Insured' status and other features against the 'Response' variable:

1. **Age Distribution by Previously Insured Status and Response**:
   - There is not a significant difference in the age distribution between those who were previously insured and not, within each response category. However, those who were not previously insured have a slightly wider age range among those interested (Response = 1).

2. **Annual Premium Distribution by Previously Insured Status and Response**:
   - The annual premium distributions are similar for both previously insured and not previously insured groups, across both response categories. There doesn’t appear to be a large difference in annual premiums that correlates with interest in new insurance.

3. **Response Distribution for Previously Insured Customers with Driving License**:
   - Nearly all customers have a driving license, so the distribution closely mirrors the overall previously insured countplot. A driving license does not seem to differentiate the response significantly.

4. **Response Distribution for Previously Insured Customers by Vehicle Damage**:
   - Customers not previously insured who have had vehicle damage are much more likely to be interested in insurance (Response = 1) than those without vehicle damage. This suggests that experience with vehicle damage is a significant factor in seeking new insurance among customers who were not previously insured.

From these insights, it's evident that 'Previously_Insured' status has a strong influence on whether customers are interested in new insurance, especially when considering their past experience with vehicle damage.

### 2.2.14 Distribution Plots based on Features
"""

def count_each_categorical_feature(train):
    categorical_columns = ['Gender', 'Age_Group', 'Region_Code_Categorical', 'Previously_Insured', 'Vehicle_Age', 'Vehicle_Damage', 'Policy_Sales_Channel_Categorical']

    fig, axes = plt.subplots(2, 7, figsize=(45, 15))
    for i in range(7):
        sns.countplot(data=train[train['Response'] == 1], x=categorical_columns[i], ax=axes[0][i])
        axes[0][i].set_xlabel(xlabel=categorical_columns[i], fontdict={'fontsize': 14})
        axes[0][i].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
        axes[0][i].set_title(categorical_columns[i], fontdict={'fontsize': 15, 'fontweight': 'bold'})

        sns.countplot(data=train[train['Response'] == 0], x=categorical_columns[i], ax=axes[1][i])
        axes[1][i].set_xlabel(xlabel=categorical_columns[i], fontdict={'fontsize': 14})
        axes[1][i].set_ylabel(ylabel='Count', fontdict={'fontsize': 14})
        axes[1][i].set_title(categorical_columns[i], fontdict={'fontsize': 15, 'fontweight': 'bold'})

    plt.suptitle('Distribution Plots based on Features', fontsize=15, fontweight='bold')
count_each_categorical_feature(train)

"""# III. DATA STANDARDIZING

## 3.1 Dropping Useless Column

- As we have already categorized 'Age', 'Region_Code', 'Annual_Premium', 'Policy_Sales_Channel', 'Vintage' features in our data set so we can now drop these features.
- We can also drop 'ID' and 'Driving_License' as they are not providing any valuable information.
"""

train.columns

# Dropping Unnecessary Columns
cols_to_drop = [ 'Age', 'Driving_License',
                'Region_Code', 'Annual_Premium',
       'Policy_Sales_Channel', 'Vintage']
train.drop(columns = cols_to_drop, inplace = True)

train

"""## 3.2 Feature Selection

### 3.2.1 Feature Classify
"""

train.dtypes

numeric_features = train.select_dtypes(include=['float64'])
categorical_features = train.select_dtypes(include=['object', 'int64'])

print("Numeric Features:")
print(numeric_features)

print("\nCategorical Features:")
print(categorical_features)

"""### 3.2.2 Numeric Feature Selection

Let's see the Kendall's correlation between numerical features.
"""

def numeric_feature_selection(df):
    plt.rcParams['figure.figsize'] = 14.7, 8.27
    numeric_features = ['Annual_Premium_Treated', 'Vintage_Treated']

    sns.heatmap(df[numeric_features].corr(method='kendall'),
                cmap="YlGnBu", annot=True)
    plt.title('Correlation Between Numeric Features', fontdict={'fontsize': 22, 'fontweight': 'bold'})

numeric_feature_selection(train)

"""- We have got two numeric features - Annual_Premium_Treated and Vintage_Treated
- There is no correlation between these two features, as a result we are going to move forward with both of them.

### 3.2.3 Categorical Feature Selection
"""

def make_features_numeric(train):
    global numeric_df
    numeric_df = train
    numeric_df['Gender'] = numeric_df['Gender'].apply(lambda x: 1 if x == 'Male' else 0)
    numeric_df['Age_Group'] = numeric_df['Age_Group'].apply(lambda x: 1 if x == 'YoungAge' else 2 if x == 'MiddleAge' else 3)
    numeric_df['Vehicle_Age'] = numeric_df['Vehicle_Age'].apply(lambda x: 1 if x == '< 1 Year' else 2 if x == '1-2 Year' else 3)
    numeric_df['Vehicle_Damage'] = numeric_df['Vehicle_Damage'].apply(lambda x: 0 if x == 'No' else 1)
    numeric_df['Policy_Sales_Channel_Categorical'] = numeric_df['Policy_Sales_Channel_Categorical'].apply(lambda x: 1 if x == 'Channel_A' else 2 if x == 'Channel_B' else 3 if x=='Channel_C' else 4)
    numeric_df['Region_Code_Categorical'] = numeric_df['Region_Code_Categorical'].apply(lambda x: 1 if x == 'Region_A' else 2 if x == 'Region_B' else 3)

make_features_numeric(train)

"""### 3.2.4 Mutual Information
Mutual information is one of many quantities that measures how much one random variables tells us about another.
"""

def mutual_info(df):
    X = df.copy()
    y = X.pop("Response")

    ros = RandomOverSampler()
    X_ros, y_ros = ros.fit_resample(X, y)
    X_ros.shape, y_ros.shape

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=0.2, random_state=42)

    high_score_features = []
    feature_scores = mutual_info_classif( X_ros, y_ros,  random_state=0)

    column_score = {}
    columns = []
    scores = []
    for score, f_name in sorted(zip(feature_scores, X_ros.columns), reverse=True):
        columns.append(f_name)
        scores.append(score)
        high_score_features.append(f_name)

    column_score['Feature'] = columns
    column_score['Score'] = scores

    return pd.DataFrame(data = column_score)

def show_feature_importance_through_mi(df):
    sns.barplot(data = mutual_info(df), x = 'Feature', y='Score')
    plt.title('Feature Importance Using Mutual Information', fontdict={'fontsize':22,'fontweight':'bold'})
    plt.xlabel('Features', fontdict={'fontsize':18})
    plt.ylabel('Score', fontdict={'fontsize':18})
    plt.xticks(rotation=90)

show_feature_importance_through_mi(numeric_df)

train.head()

"""# IV. MODELING USINGMACHINE LEARNING - RATIO 80:20

## 4.1 Split Train.CSV into Train and Valid Dataset

### 4.1.1 Modeling Response Distrribution Percentage
"""

class_distribution = train['Response'].value_counts(normalize=True) * 100

colors = ['skyblue', 'lightgreen', 'lightcoral']

plt.figure(figsize=(8, 6))
bars = plt.bar(class_distribution.index, class_distribution.values, color=colors)
plt.xlabel('Class')
plt.ylabel('Percentage (%)')
plt.title('Response Distribution')

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}%', ha='center', va='bottom')

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
plt.pie(class_distribution, labels=class_distribution.index, autopct='%1.1f%%', startangle=140, colors=colors)
plt.axis('equal')
plt.title('Response Distribution')
plt.show()

"""- In conclusion, the data is imbalanced. The results indicate that 87.74% of the customers are interested (1), while 12.26% of the customers are not interested (0).

### 4.1.2 Seperate Target Column ( Response)
"""

target_column = 'Response'
y = train[target_column]
X = train.drop(target_column, axis=1)

"""### 4.1.3 Using the method that makes no changes to the data.

- In all the tests below, to determine the optimal method for splitting the training and validation datasets for our 'train.csv' dataset, we utilize the BaggingClassifier to evaluate the accuracy of each method.
- After conducting the tests and obtaining the results, we will convert the code into markdown format if we achieve the highest accuracy percentages.
- We will use ratio all among the test below is 80:20
"""



"""#### a) Method #1 ( Normal way)"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_nmw(X, y, n_estimators=50, test_size=0.2, random_state=42):
    base_classifier = BaggingClassifier()
    start_time = time.time()

    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)

    # Initialize the base classifier as DecisionTreeClassifier
    bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')

    # Predict the probabilities for each class
    y_pred_proba = bagging_classifier.predict_proba(X_valid)

    # Calculate log loss
    logloss = log_loss(y_valid, y_pred_proba)

    # Tính confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' %accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % logloss)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

    return accuracy, f1, logloss

# Gọi hàm và lưu giá trị log loss vào biến
accuracy, f1, logloss = result_train_and_valid_split_nmw(X_copy, y_copy, n_estimators=50, test_size=0.2, random_state=42)

"""- Accuracy using BaggingClassifier: 0.862
- F1 Score using BaggingClassifier: 0.541
- Log Loss using BaggingClassifier: 0.316
- Thời gian thực thi: 00:05:28

#### b) Method #2 ( Stratified K-Fold way)
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_Stratified_KFold(X, y, n_estimators=50, n_splits=5, random_state=42):
    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Convert X and y to numpy arrays for indexing
    X = X.values
    y = y.values

    # Initialize the Stratified K-fold cross-validator
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    # Initialize the base classifier
    base_classifier = BaggingClassifier()

    start_time = time.time()  # Bắt đầu tính thời gian thực thi

    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Create and fit the bagging classifier
        bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
        bagging_classifier.fit(X_train, y_train)

        # Predict and calculate metrics
        y_pred = bagging_classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='macro')
        cm = confusion_matrix(y_test, y_pred)

        accuracy_scores.append(accuracy)
        f1_scores.append(f1)
        confusion_matrices.append(cm)

        # Tính và lưu log loss
        y_pred_proba = bagging_classifier.predict_proba(X_test)
        logloss = log_loss(y_test, y_pred_proba)
        log_losses.append(logloss)

    # Calculate mean accuracy and F1 score
    mean_accuracy = np.mean(accuracy_scores)
    mean_f1 = np.mean(f1_scores)

    # Calculate mean confusion matrix
    mean_confusion_matrix = np.mean(confusion_matrices, axis=0)

    # Calculate mean log loss
    mean_log_loss = np.mean(log_losses)

    end_time = time.time()  # Kết thúc tính thời gian thực thi
    execution_time = end_time - start_time

    # Vẽ mean confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(mean_confusion_matrix, annot=True, fmt='.2f', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Mean Confusion Matrix')
    plt.show()

    print('Mean Accuracy using BaggingClassifier and Stratified K-Fold: %.3f' % mean_accuracy)
    print('Mean F1 Score using BaggingClassifier and Stratified K-Fold: %.3f' % mean_f1)
    print('Mean Log Loss using BaggingClassifier and Stratified K-Fold: %.3f' % mean_log_loss)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_train_and_valid_split_Stratified_KFold để đánh giá mô hình
result_train_and_valid_split_Stratified_KFold(X_copy, y_copy, n_estimators=50, n_splits=5, random_state=42)

"""- Mean Accuracy using BaggingClassifier and Stratified K-Fold: 0.863
- Mean F1 Score using BaggingClassifier and Stratified K-Fold: 0.541
- Mean Log Loss using BaggingClassifier and Stratified K-Fold: 0.313
- Thời gian thực thi: 00:38:39

#### c) Method #3 (K-fold way)
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_KFold(X, y, n_estimators=50, n_splits=5, random_state=42):
    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Convert X and y to numpy arrays for indexing
    X = X.values
    y = y.values

    # Initialize the K-fold cross-validator
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    base_classifier = BaggingClassifier()
    start_time = time.time()  # Bắt đầu tính thời gian thực thi

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Create and fit the bagging classifier
        bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
        bagging_classifier.fit(X_train, y_train)

        # Predict and calculate metrics
        y_pred = bagging_classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='macro')
        log_loss_val = log_loss(y_test, bagging_classifier.predict_proba(X_test))  # Tính log loss

        accuracy_scores.append(accuracy)
        f1_scores.append(f1)
        log_losses.append(log_loss_val)
        confusion_matrices.append(confusion_matrix(y_test, y_pred))

    # Calculate mean accuracy, F1 score, and log loss
    mean_accuracy = np.mean(accuracy_scores)
    mean_f1 = np.mean(f1_scores)
    mean_log_loss = np.mean(log_losses)  # Log loss trung bình
    mean_confusion_matrix = np.mean(confusion_matrices, axis=0)

    end_time = time.time()  # Kết thúc tính thời gian thực thi
    execution_time = end_time - start_time

    # Vẽ mean confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(mean_confusion_matrix, annot=True, fmt='.2f', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Mean Confusion Matrix')
    plt.show()

    print('Mean Accuracy using BaggingClassifier and K-Fold: %.3f' % mean_accuracy)
    print('Mean F1 Score using BaggingClassifier and K-Fold: %.3f' % mean_f1)
    print('Mean Log Loss using BaggingClassifier and K-Fold: %.3f' % mean_log_loss)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm để đánh giá mô hình
result_train_and_valid_split_KFold(X_copy, y_copy, n_estimators=50, n_splits=5, random_state=42)

"""- Mean Accuracy using BaggingClassifier and K-Fold: 0.863
- Mean F1 Score using BaggingClassifier and K-Fold: 0.542
- Mean Log Loss using BaggingClassifier and K-Fold: 0.312
- Thời gian thực thi: 00:38:21

### 4.1.4 Using the method that makes changes to the data. (KA)

#### a) Oversampling using SMOTE
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_SMOTE(X, y, n_estimators=50, test_size=0.2, random_state=42):
    # Sử dụng SMOTE để oversample dữ liệu
    sm = SMOTE()
    X_sm, y_sm = sm.fit_resample(X_copy, y_copy)

    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    base_classifier = BaggingClassifier()
    start_time = time.time()

    # Train và validate
    X_train, X_valid, y_train, y_valid = train_test_split(X_sm, y_sm, test_size=test_size, stratify=y_sm, random_state=random_state)

    bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, bagging_classifier.predict_proba(X_valid))  # Tính log loss

    # Tính confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    accuracy_scores.append(accuracy)
    f1_scores.append(f1)
    log_losses.append(log_loss_val)
    confusion_matrices.append(cm)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' % accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % log_loss_val)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm để đánh giá mô hình
result_train_and_valid_split_SMOTE(X_copy, y_copy, n_estimators=50, test_size=0.2, random_state=42)

"""- Accuracy using BaggingClassifier: 0.856
- F1 Score using BaggingClassifier: 0.856
- Log Loss using BaggingClassifier: 0.323
- Thời gian thực thi: 00:28:12

#### b) Undersampling using NearMiss
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_Nearmiss(X, y, n_estimators=50, test_size=0.2, random_state=42):
    # Áp dụng NearMiss để xử lý mất cân bằng dữ liệu
    nm = NearMiss()
    X_res, y_res = nm.fit_resample(X_copy, y_copy)
    X_res.shape, y_res.shape

    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    base_classifier = BaggingClassifier()
    start_time = time.time()

    X_train, X_valid, y_train, y_valid = train_test_split(X_res, y_res, test_size= test_size, random_state= random_state)

    bagging_classifier = BaggingClassifier(base_classifier, n_estimators= n_estimators, random_state=random_state)

    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, bagging_classifier.predict_proba(X_valid))  # Tính log loss

    #Tính Confusion Matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' %accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % log_loss_val)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm để chạy
result_train_and_valid_split_Nearmiss(X_copy, y_copy, n_estimators=50, test_size=0.2, random_state=42)

"""- Accuracy using BaggingClassifier: 0.652
- F1 Score using BaggingClassifier: 0.651
- Log Loss using BaggingClassifier: 0.655
- Thời gian thực thi: 00:01:57

#### c) Random Oversampling
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_RandomOverSampler(X, y, n_estimators=50, test_size=0.2, random_state=42):
    # Áp dụng RandomOverSampler để xử lý mất cân bằng dữ liệu
    ros = RandomOverSampler()
    X_ros, y_ros = ros.fit_resample(X_copy, y_copy)
    X_ros.shape, y_ros.shape

    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    base_classifier = BaggingClassifier()
    start_time = time.time()

    X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=test_size, random_state=random_state)

    # Khởi tạo base_classifier

    bagging_classifier = BaggingClassifier(base_classifier, n_estimators= n_estimators, random_state=random_state)
    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, bagging_classifier.predict_proba(X_valid))  # Tính log loss

    # Tính Confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' % accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % log_loss_val)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm để chạy
result_train_and_valid_split_RandomOverSampler(X_copy, y_copy, n_estimators=50, test_size=0.2, random_state=42)

"""- Accuracy using BaggingClassifier: 0.908
- F1 Score using BaggingClassifier: 0.907
- Log Loss using BaggingClassifier: 0.214
- Thời gian thực thi: 00:15:54

So after comparisions, we found out that Random Oversampling is the best option

### 4.1.5 OUR METHOD
"""

ros = RandomOverSampler()
X_ros, y_ros = ros.fit_resample(X, y)
X_ros.shape, y_ros.shape

# Train và kiểm tra mô hình trên dữ liệu đã xử lý
X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=0.2, random_state=42)

X_train.head()

X_valid.head()

y_train.head()

y_valid.head()

"""## 4.2 Machine learning (No Hyper Parameter Tuning)

### 4.2.1 Decision Tree
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_DecisionTree(X_train, X_valid, y_train, y_valid, random_state=42):
    start_time = time.time()

    # Bổ sung hyperparameters cho DecisionTreeClassifier
    clf = DecisionTreeClassifier(
        criterion='gini',  # Loại criterion: 'gini' hoặc 'entropy'
        splitter='best',  # Chọn 'best' hoặc 'random'
        max_depth=None,  # Độ sâu tối đa của cây
        min_samples_split=2,  # Số mẫu tối thiểu để chia một node nếu chưa đạt max_depth
        min_samples_leaf=1,  # Số mẫu tối thiểu trong các lá
        min_weight_fraction_leaf=0.0,
        max_features=None,  # Số tính năng tối đa để xem xét khi tìm kiếm phân chia tốt nhất
        random_state=random_state
    )
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using DecisionTreeClassifier: %.3f' % accuracy)
    print('F1 Score using DecisionTreeClassifier: %.3f' % f1)
    print('Log Loss using DecisionTreeClassifier: %.3f' % log_loss_val)
    print('AUC using DecisionTreeClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm
result_DecisionTree(X_train_copy, X_valid_copy, y_train_copy, y_valid_copy, random_state=42)

"""- Accuracy using DecisionTreeClassifier: 0.931
- F1 Score using DecisionTreeClassifier: 0.931
- Log Loss using DecisionTreeClassifier: 1.885
- AUC using DecisionTreeClassifier: 0.941
- Thời gian thực thi: 00:00:06

### 4.2.2 Logistic Regression
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_LogisticRegression(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình Logistic Regression và cài đặt các hyperparameters
    clf = LogisticRegression(
        penalty='l2',  # Loại penalty, có thể là 'l1' hoặc 'l2'
        C=1.0,  # Tham số điều chỉnh độ mạnh của regularization (1.0 là giá trị mặc định)
        solver='lbfgs',  # Thuật toán tối ưu hóa, có thể là 'lbfgs', 'liblinear', 'sag', 'saga'
        max_iter=100,  # Số lần lặp tối đa
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Logistic Regression')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    logloss = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using Logistic Regression: %.3f' % accuracy)
    print('F1 Score using Logistic Regression: %.3f' % f1)
    print('Log Loss using Logistic Regression: %.3f' % logloss)
    print('AUC using Logistic Regression: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_LogisticRegression() với dữ liệu đã chia sẵn
result_LogisticRegression(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using Logistic Regression: 0.787
- F1 Score using Logistic Regression: 0.781
- Log Loss using Logistic Regression: 0.435
- AUC using Logistic Regression: 0.842
- Thời gian thực thi: 00:00:03

### 4.2.3 Bagging Classifier
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_BaggingClassifier(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình BaggingClassifier và cài đặt các hyperparameters
    clf = BaggingClassifier(
        n_estimators=50,  # Số lượng cây quyết định trong ensemble
        random_state=42,  # Seed cho mô hình
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - BaggingClassifier')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    logloss = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using BaggingClassifier: %.3f' % accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % logloss)
    print('AUC using BaggingClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm với dữ liệu tương ứng
result_BaggingClassifier(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using BaggingClassifier: 0.932
- F1 Score using BaggingClassifier: 0.931
- Log Loss using BaggingClassifier: 0.262
- AUC using BaggingClassifier: 0.989
- Thời gian thực thi: 00:02:52

### 4.2.4. Random Forest
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_RandomForestClassifier(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình RandomForestClassifier và cài đặt các hyperparameters
    clf = RandomForestClassifier(
        n_estimators=50,  # Số lượng cây quyết định trong ensemble
        random_state=42,  # Seed cho mô hình
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - RandomForestClassifier')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    logloss = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using RandomForestClassifier: %.3f' % accuracy)
    print('F1 Score using RandomForestClassifier: %.3f' % f1)
    print('Log Loss using RandomForestClassifier: %.3f' % logloss)
    print('AUC using RandomForestClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_RandomForestClassifier() với dữ liệu đã chia sẵn
result_RandomForestClassifier(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using RandomForestClassifier: 0.928
- F1 Score using RandomForestClassifier: 0.928
- Log Loss using RandomForestClassifier: 0.318
- AUC using RandomForestClassifier: 0.985
- Thời gian thực thi: 00:01:05

### 4.2.5 XGBoost
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_XGBoostClassifier(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình XGBoostClassifier và cài đặt các hyperparameters
    clf = xgb.XGBClassifier(
        learning_rate=0.1,  # Tốc độ học
        nthread=-1,  # Số lượng luồng sử dụng (tự động sử dụng tất cả luồng có sẵn)
        max_depth=3,  # Độ sâu tối đa của cây
        colsample_bytree=1.0,  # Tỷ lệ mẫu đặc trưng khi xây dựng mỗi cây
        reg_lambda=1.0,  # Hệ số regularization L2
        reg_alpha=0.0  # Hệ số regularization L1
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Tính độ chính xác (accuracy)
    accuracy = accuracy_score(y_valid, y_pred)

    # Tính F1 score
    f1 = f1_score(y_valid, y_pred, average='macro')

    # Tính log loss
    y_pred_proba = clf.predict_proba(X_valid)
    logloss = log_loss(y_valid, y_pred_proba)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    # Tính ROC curve và AUC
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba[:, 1])
    auc_score = auc(fpr, tpr)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - XGBoostClassifier')
    plt.legend(loc='lower right')
    plt.show()

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    end_time = time.time()
    execution_time = end_time - start_time

    print('Accuracy using XGBoostClassifier: %.3f' % accuracy)
    print('F1 Score using XGBoostClassifier: %.3f' % f1)
    print('Log Loss using XGBoostClassifier: %.3f' % logloss)
    print('AUC using XGBoostClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_XGBoostClassifier() với dữ liệu đã chia sẵn
result_XGBoostClassifier(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using XGBoostClassifier: 0.790
- F1 Score using XGBoostClassifier: 0.785
- Log Loss using XGBoostClassifier: 0.431
- AUC using XGBoostClassifier: 0.845
- Thời gian thực thi: 00:00:06

### 4.2.6 Voting Classifier
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_SoftVoting(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo các mô hình con
    decision_tree = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None,
                                           min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
                                           max_features=None, random_state=42)
    logistic_regression = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100)
    bagging_classifier = BaggingClassifier(n_estimators=50, random_state=42)

    # Khởi tạo mô hình Soft Voting
    voting_clf = VotingClassifier(
        estimators=[('dt', decision_tree), ('lr', logistic_regression), ('bagging', bagging_classifier)],
        voting='soft'  # Soft Voting
    )

    # Huấn luyện mô hình Soft Voting
    voting_clf.fit(X_train, y_train)

    # Dự đoán xác suất cho từng lớp
    y_pred_proba = voting_clf.predict_proba(X_valid)

    # Dự đoán lớp (chọn lớp có xác suất cao nhất)
    y_pred = np.argmax(y_pred_proba, axis=1)

    # Tính độ chính xác (accuracy)
    accuracy = accuracy_score(y_valid, y_pred)

    # Tính F1 score
    f1 = f1_score(y_valid, y_pred, average='macro')

    # Tính log loss
    logloss = log_loss(y_valid, y_pred_proba)

    # Tính ROC curve và AUC
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba[:, 1])
    auc_score = auc(fpr, tpr)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Soft Voting')
    plt.legend(loc='lower right')
    plt.show()

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    end_time = time.time()
    execution_time = end_time - start_time

    print('Accuracy using Soft Voting: %.3f' % accuracy)
    print('F1 Score using Soft Voting: %.3f' % f1)
    print('Log Loss using Soft Voting: %.3f' % logloss)
    print('AUC using Soft Voting: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm với dữ liệu tương ứng
result_SoftVoting(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using Soft Voting: 0.930
- F1 Score using Soft Voting: 0.930
- Log Loss using Soft Voting: 0.221
- AUC using Soft Voting: 0.982
- Thời gian thực thi: 00:02:50

## 4.3 Hyper Parameter Tuning

### 4.3.1 Def Function Definition
"""

def plot_confusion_matrix_and_roc_curves(model, X_valid, y_valid, y_pred):

    fig, axes = plt.subplots(1,2, figsize=(22,5))

    cm = confusion_matrix(y_valid, y_pred)
    group_names = ['True Neg','False Pos','False Neg','True Pos']
    group_counts = ['{0:0.0f}'.format(value) for value in
                    cm.flatten()]
    group_percentages = ['{0:.2%}'.format(value) for value in
                        cm.flatten()/np.sum(cm)]
    labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
            zip(group_names,group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)

    sns.heatmap(cm, ax = axes[0], annot=labels, fmt='',cmap='Blues')
    axes[0].set_title('Confusion Matrix', fontdict={'fontsize': 16, 'fontweight':'bold'})

    # predict probabilities
    pred_proba = model.predict_proba(X_valid)

    # roc curve for models
    fpr, tpr, thresh = roc_curve(y_valid, pred_proba[:,1], pos_label=1)

    # roc curve for tpr = fpr
    random_probs = [0 for i in range(len(y_valid))]
    p_fpr, p_tpr, _ = roc_curve(y_valid, random_probs, pos_label=1)

    plt.subplot(1, 2, 2)
    # plot roc curves
    plt.plot(fpr, tpr,linestyle='--',color='red', label = type(model).__name__)

    plt.plot(p_fpr, p_tpr, linestyle='-', color='blue')
    # title
    plt.title('ROC curve', fontdict={'fontsize': 16, 'fontweight':'bold'})
    # x label
    plt.xlabel('False Positive Rate', fontdict={'fontsize': 12})
    # y label
    plt.ylabel('True Positive rate', fontdict={'fontsize': 12})

    plt.legend(loc='best')
    plt.show()

def visualization(results_df, parameters):

    def shorten_param(param_name):
        if "__" in param_name:
            return param_name.rsplit("__", 1)[1]
        return param_name

    column_results = [f"param_{name}" for name in parameters.keys()]
    column_results += ["mean_test_score", "std_test_score", "rank_test_score"]

    results_df = results_df[column_results].sort_values("mean_test_score", ascending=False)
    results_df = results_df.rename(shorten_param, axis=1)

    for col in results_df.columns:
        if col == 'param_random_state':
            continue
        try:
            results_df[col] = results_df[col].astype(np.float64)
        except:
            continue

    fig = px.parallel_coordinates(
    results_df,
    color="mean_test_score",
    color_continuous_scale=px.colors.sequential.Viridis,
    title='Hyper Parameter Tuning',)
    fig.show()

def evaluation_metrics(name, independent_feature_length , y_pred, y_valid):

    metrics_dict = {}
    metrics_dict['Accuracy_Score'] = [accuracy_score(y_valid,y_pred)]  #Accuracy Score
    metrics_dict['Precision'] = [precision_score(y_valid,y_pred)] #Precision
    metrics_dict['Recall'] = [recall_score(y_valid,y_pred)] #Recall
    metrics_dict['F1_Score'] = [f1_score(y_valid,y_pred)] #F1 Score
    metrics_dict['ROC_AUC_Score'] = [roc_auc_score(y_valid, y_pred)] #ROC AUC Score
    metrics_dict['Log_Loss'] = [log_loss(y_valid, y_pred)] #Log Loss

    metrics_df = pd.DataFrame(metrics_dict)

    print(metrics_df)

def hyperparameter_tuning(X_train, y_train, model, parameters, tuning_model):

    if tuning_model == 'Halving_Randomized_Search_CV':
        tuned_model = HalvingRandomSearchCV(model, param_distributions=parameters, scoring="accuracy", n_jobs=-1, factor=3, cv=5)

    elif tuning_model == 'Randomized_Search_CV':
        tuned_model = RandomizedSearchCV(model, param_distributions=parameters, scoring='accuracy', cv=3, n_iter=50, n_jobs=-1)

    elif tuning_model == 'Grid_Search_CV':
        tuned_model = GridSearchCV(model, param_grid=parameters, scoring='accuracy', n_jobs=-1, cv=3)

    elif tuning_model == 'RandomSampler':
        sampler = random_sample(seed=42)  # Chọn seed tùy ý
        study = optuna.create_study(sampler=sampler, direction='maximize')
        optuna_model = optuna.integration.OptunaSearchCV(model, param_distributions=parameters, n_jobs=-1, cv=3, study=study, verbose=1)
        tuned_model = optuna_model

    elif tuning_model == 'TPE':
        sampler = TPESampler(seed=42)  # Chọn seed tùy ý
        study = optuna.create_study(sampler=sampler, direction='maximize')
        optuna_model = optuna.integration.OptunaSearchCV(model, param_distributions=parameters, n_jobs=-1, cv=3, study=study, verbose=1)
        tuned_model = optuna_model

    start_time = time.time()

    tuned_model.fit(X_train, y_train)

    stop_time = time.time()

    print('*****'*10+f'\nBest Score for {type(model).__name__} : {tuned_model.best_score_}','\n---')
    print(f'Best Parameters for {type(model).__name__} : {tuned_model.best_params_}\n'+'-----'*10)

    print('Elapsed Time:',time.strftime("%H:%M:%S", time.gmtime(stop_time - start_time)))
    print('======'*5)

    return tuned_model

def perform_ml_algorithm(X_train, X_valid, y_train, y_valid, model, parameters, tuning_model):
    print('-----'*10+f'\n{type(model).__name__}\n'+'-----'*10)

    model.fit(X_train, y_train)
    untuned_pred = model.predict(X_valid)

    # Evaluation Metrics before tuning
    print(f'\nEvaluation of {type(model).__name__} before tuning:\n'+'-----'*10)
    evaluation_metrics(type(model).__name__, len(list(X_train.columns)), untuned_pred, y_valid)

    print()
    plot_confusion_matrix_and_roc_curves(model, X_valid, y_valid, untuned_pred)

    # Hyper-parameter tuning
    tuned_model = hyperparameter_tuning(X_train, y_train, model, parameters, tuning_model)
    tuned_pred = tuned_model.predict(X_valid)

    # Evaluation Metrics after tuning
    print(f'\nEvaluation of {type(model).__name__} after tuning:\n'+'-----'*10)
    evaluation_metrics(type(model).__name__,len(list(X_train.columns)), tuned_pred, y_valid)

    print()
    plot_confusion_matrix_and_roc_curves(tuned_model.best_estimator_, X_valid, y_valid, tuned_pred)
    visualization(pd.DataFrame(tuned_model.cv_results_), parameters)

def ml_algorithm_implementation(df, model, parameters, tuning_model, feature_importance = False):

    if feature_importance == False:
        print('########'*8+'\n     <<<< '+f'Tuning Model: {tuning_model}'+' >>>>\n'+'********'*8)

    x = train.iloc[:,1:]
    y = train['Response']

    # Train Test Split
    ros = RandomOverSampler()
    X_ros, y_ros = ros.fit_resample(X, y)
    X_ros.shape, y_ros.shape

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=0.2, random_state=42)

    if feature_importance == True:
        model.fit(X_train, y_train)
        return X_train, y_train, model

    perform_ml_algorithm(X_train, X_valid, y_train, y_valid, model, parameters, tuning_model)

'''
def ml_algorithm_implementation_optuna(data_df, model, parameters, tuning_model, feature_importance=False):
    if feature_importance is False:
        print('########' * 8 + '\n     <<<< ' + f'Tuning Model: {tuning_model}' + ' >>>>\n' + '********' * 8)

    x = data_df.iloc[:, 1:]
    y = data_df['Response']

    # Train Test Split
    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=57)

    if feature_importance is True:
        model.fit(x_train, y_train)
        return x_train, y_train, model

    # Create an Optuna study and optimize the hyperparameters
    study = optuna.create_study(direction='maximize', sampler=TPESampler())
    study.optimize(objective, n_trials=100)

    # Get the best parameters after optimization
    best_params = study.best_params

    # Create a new BaggingClassifier with the best parameters
    best_n_estimators = best_params['n_estimators']
    best_max_samples = best_params['max_samples']
    best_model = BaggingClassifier(n_estimators=best_n_estimators, max_samples=best_max_samples, random_state=42)

    # Hyper-parameter tuning
    tuned_model = hyperparameter_tuning(x_train, y_train, best_model, parameters, tuning_model)

    # Predict on test data
    tuned_pred = tuned_model.predict(x_test)

    # Evaluation Metrics after tuning
    print(f'\nEvaluation of {type(model).__name__} after tuning:\n' + '-----' * 10)
    evaluation_metrics(type(model).__name__, len(list(x_train.columns)), tuned_pred, y_test)

    print()
    plot_confusion_matrix_and_roc_curves(tuned_model.best_estimator_, x_test, y_test, tuned_pred)
    visualization(pd.DataFrame(tuned_model.cv_results_), parameters)

    return study
'''

"""### 4.3.2 Main"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_bagging = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

Tuning_Method = 'Randomized_Search_CV'
parameters_bagging = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

Tuning_Method = 'Grid_Search_CV'
parameters_bagging = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

'''
Tuning_Method = 'RandomSampler'

# Define the model and parameters
model = BaggingClassifier()
parameters = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

# Create the Optuna study and optimize the hyperparameters
study = ml_algorithm_implementation_optuna(train, model, parameters, Tuning_Method, feature_importance=False)
'''

'''
Tuning_Method = 'TPE'

def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 10, 400)
    max_samples = trial.suggest_float('max_samples', 0.5, 1.0)
    model = BaggingClassifier(n_estimators=n_estimators, max_samples=max_samples, random_state=42)

ml_algorithm_implementation(train, BaggingClassifier(), objective, Tuning_Method, False)
ml_algorithm_implementation_optuna(data_df, model, parameters, tuning_model, feature_importance=False)
'''

"""## 4.4 Machine learning (Hyper Parameter Tuning)

### 4.4.1 Decision Tree
"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_decision_tree = {"splitter":["best","random"],
            "max_depth" : [None,5,7,9],
           "min_samples_leaf":[1,2,3,4,5],
           "min_weight_fraction_leaf":[0.0, 0.3,0.4,0.5],
           "max_features":["auto","log2","sqrt",None],
           "max_leaf_nodes":[None,30,40,50,60],
           'random_state':[42]}
ml_algorithm_implementation(train, DecisionTreeClassifier(), parameters_decision_tree, Tuning_Method, False)

"""### 4.4.2 Logistic Regression"""

# Set the tuning method
Tuning_Method = 'Halving_Randomized_Search_CV'

# Define the parameters for the logistic regression model
parameters_logistic = {'solver' : ['newton-cg', 'lbfgs', 'liblinear','sag','saga'],
                        'penalty' : ['l2'],
                        'C' : [100, 10, 1.0, 0.1, 0.01, 0.001],
                       'random_state':[42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, LogisticRegression(), parameters_logistic, Tuning_Method, False)

"""### 4.4.3 Bagging Classifier"""

Tuning_Method = 'Halving_Randomized_Search_CV'

parameters_bagging = {'n_estimators':[10, 100, 200, 400],
                      'random_state':[42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

"""### 4.4.4 XGBoost"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_xgb = {'n_estimators': [100, 200, 400],
                  'max_depth': [3, 5, 7],
                  'learning_rate': [0.01, 0.1, 0.3],
                  'random_state': [42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, xgb.XGBClassifier(), parameters_xgb, Tuning_Method, False)

"""### 4.4.5 Random Forest"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_random_forest = {'n_estimators': [100, 200, 400],
                            'max_depth': [3, 5, 7],
                            'random_state': [42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, RandomForestClassifier(), parameters_random_forest, Tuning_Method, False)

"""### 4.4.6 Voting Classifier"""

Tuning_Method = 'Halving_Randomized_Search_CV'
# Khởi tạo các mô hình con
decision_tree = DecisionTreeClassifier(max_depth=3)
logistic_regression = LogisticRegression()
bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)

# Khởi tạo mô hình Soft Voting
voting_clf = VotingClassifier(
    estimators=[('dt', decision_tree), ('lr', logistic_regression), ('bagging', bagging_classifier)],
    voting='soft'  # Chuyển sang Soft Voting
)

# Thiết lập hyperparameters bạn muốn tinh chỉnh (ở đây, chỉ có ví dụ cho decision_tree)
parameters_soft_voting = {
    'dt__max_depth': [3, 5, 7],
    'dt__min_samples_split': [2, 4, 6],
    # Thêm các hyperparameters cho các mô hình con khác nếu cần
}

# Run the Optuna hyperparameter tuning cho Soft Voting
ml_algorithm_implementation(train, voting_clf, parameters_soft_voting, Tuning_Method, False)

"""## 4.5 Extracting Feature Importance"""

def feature_plot(importances, X_train, y_train):

    # Display the five most important features
    indices = np.argsort(importances)[::-1]
    columns = X_train.columns.values[indices[:5]]
    values = importances[indices][:5]

    # Creat the plot
    fig = plt.figure(figsize = (9,5))
    plt.title("Normalized Weights for First Five Most Predictive Features", fontsize = 16)
    plt.bar(np.arange(5), values, width = 0.2, align="center", color = '#00A000', \
          label = "Feature Weight")
    plt.bar(np.arange(5) - 0.2, np.cumsum(values), width = 0.2, align = "center", color = '#00A0A0', \
          label = "Cumulative Feature Weight")
    plt.xticks(np.arange(5), columns)
    plt.xlim((-0.5, 4.5))
    plt.ylabel("Weight", fontsize = 12)
    plt.xlabel("Feature", fontsize = 12)

    plt.legend(loc = 'upper center')
    plt.tight_layout()
    plt.show()

def show_feature_importance():
    x_train, y_train, model = ml_algorithm_implementation(train, BaggingClassifier(n_estimators=200, random_state=42),
                                None, None, True)

    importances = np.mean([
        tree.feature_importances_ for tree in model.estimators_
        ], axis=0)
    feature_plot(importances, x_train, y_train)

show_feature_importance()

# Define the classifier to use
dtree = DecisionTreeClassifier()

# Set up the parameter grid
parameters_decision_tree = {
    "splitter": ["best", "random"],
    "max_depth": [None, 5, 7, 9],
    "min_samples_leaf": [1, 2, 3, 4, 5],
    "min_weight_fraction_leaf": [0.0, 0.3, 0.4, 0.5],
    "max_features": ["auto", "log2", "sqrt", None],
    "max_leaf_nodes": [None, 30, 40, 50, 60],
    'random_state': [42]
}

# Set up the grid search
grid_search = GridSearchCV(estimator=dtree, param_grid=parameters_decision_tree, cv=5, n_jobs=-1, scoring='accuracy')

# Fit the grid search to the data
grid_search.fit(X, y)  # X and y should be your data features and target variable

# Get the best parameters
best_parameters = grid_search.best_params_

"""# VI. Machine Learning ( ratio 70-30)

## 6.1 Split Train.CSV into Train and Valid Dataset

### 6.1.2 Seperate Target Column ( Response)
"""

target_column = 'Response'
y = train[target_column]
X = train.drop(target_column, axis=1)

"""### 6.1.3 Using the method that makes no changes to the data.

- In all the tests below, to determine the optimal method for splitting the training and validation datasets for our 'train.csv' dataset, we utilize the BaggingClassifier to evaluate the accuracy of each method.
- After conducting the tests and obtaining the results, we will convert the code into markdown format if we achieve the highest accuracy percentages.
- We will use ratio all among the test below is 70:30

#### a) Method #1 (Normal way)
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_nmw(X, y, n_estimators=50, test_size=0.3, random_state=42):
    base_classifier = BaggingClassifier()
    start_time = time.time()

    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)

    # Initialize the base classifier as DecisionTreeClassifier
    bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')

    # Predict the probabilities for each class
    y_pred_proba = bagging_classifier.predict_proba(X_valid)

    # Calculate log loss
    logloss = log_loss(y_valid, y_pred_proba)

    # Tính confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' %accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % logloss)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

    return accuracy, f1, logloss

# Gọi hàm và lưu giá trị log loss vào biến
accuracy, f1, logloss = result_train_and_valid_split_nmw(X_copy, y_copy, n_estimators=50, test_size=0.3, random_state=42)

"""- Accuracy using BaggingClassifier: 0.863
- F1 Score using BaggingClassifier: 0.541
- Log Loss using BaggingClassifier: 0.316
- Thời gian thực thi: 00:07:43

#### b) Method #2 ( Stratified K-Fold way)
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_Stratified_KFold(X, y, n_estimators=50, n_splits=10, random_state=42):
    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Convert X and y to numpy arrays for indexing
    X = X.values
    y = y.values

    # Initialize the Stratified K-fold cross-validator
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    # Initialize the base classifier
    base_classifier = BaggingClassifier()

    start_time = time.time()  # Bắt đầu tính thời gian thực thi

    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Create and fit the bagging classifier
        bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
        bagging_classifier.fit(X_train, y_train)

        # Predict and calculate metrics
        y_pred = bagging_classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='macro')
        cm = confusion_matrix(y_test, y_pred)

        accuracy_scores.append(accuracy)
        f1_scores.append(f1)
        confusion_matrices.append(cm)

        # Tính và lưu log loss
        y_pred_proba = bagging_classifier.predict_proba(X_test)
        logloss = log_loss(y_test, y_pred_proba)
        log_losses.append(logloss)

    # Calculate mean accuracy and F1 score
    mean_accuracy = np.mean(accuracy_scores)
    mean_f1 = np.mean(f1_scores)

    # Calculate mean confusion matrix
    mean_confusion_matrix = np.mean(confusion_matrices, axis=0)

    # Calculate mean log loss
    mean_log_loss = np.mean(log_losses)

    end_time = time.time()  # Kết thúc tính thời gian thực thi
    execution_time = end_time - start_time

    # Vẽ mean confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(mean_confusion_matrix, annot=True, fmt='.2f', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Mean Confusion Matrix')
    plt.show()

    print('Mean Accuracy using BaggingClassifier and Stratified K-Fold: %.3f' % mean_accuracy)
    print('Mean F1 Score using BaggingClassifier and Stratified K-Fold: %.3f' % mean_f1)
    print('Mean Log Loss using BaggingClassifier and Stratified K-Fold: %.3f' % mean_log_loss)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_train_and_valid_split_Stratified_KFold để đánh giá mô hình
result_train_and_valid_split_Stratified_KFold(X_copy, y_copy, n_estimators=50, n_splits=10, random_state=42)

"""- Mean Accuracy using BaggingClassifier and Stratified K-Fold: 0.863
- Mean F1 Score using BaggingClassifier and Stratified K-Fold: 0.542
- Mean Log Loss using BaggingClassifier and Stratified K-Fold: 0.315
- Thời gian thực thi: 01:23:18

#### c) Method #3 (K-fold way)
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_KFold(X, y, n_estimators=50, n_splits=10, random_state=42):
    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Convert X and y to numpy arrays for indexing
    X = X.values
    y = y.values

    # Initialize the K-fold cross-validator
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    base_classifier = BaggingClassifier()
    start_time = time.time()  # Bắt đầu tính thời gian thực thi

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Create and fit the bagging classifier
        bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
        bagging_classifier.fit(X_train, y_train)

        # Predict and calculate metrics
        y_pred = bagging_classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='macro')
        log_loss_val = log_loss(y_test, bagging_classifier.predict_proba(X_test))  # Tính log loss

        accuracy_scores.append(accuracy)
        f1_scores.append(f1)
        log_losses.append(log_loss_val)
        confusion_matrices.append(confusion_matrix(y_test, y_pred))

    # Calculate mean accuracy, F1 score, and log loss
    mean_accuracy = np.mean(accuracy_scores)
    mean_f1 = np.mean(f1_scores)
    mean_log_loss = np.mean(log_losses)  # Log loss trung bình
    mean_confusion_matrix = np.mean(confusion_matrices, axis=0)

    end_time = time.time()  # Kết thúc tính thời gian thực thi
    execution_time = end_time - start_time

    # Vẽ mean confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(mean_confusion_matrix, annot=True, fmt='.2f', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Mean Confusion Matrix')
    plt.show()

    print('Mean Accuracy using BaggingClassifier and K-Fold: %.3f' % mean_accuracy)
    print('Mean F1 Score using BaggingClassifier and K-Fold: %.3f' % mean_f1)
    print('Mean Log Loss using BaggingClassifier and K-Fold: %.3f' % mean_log_loss)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm để đánh giá mô hình
result_train_and_valid_split_KFold(X_copy, y_copy, n_estimators=50, n_splits=10, random_state=42)

"""- Mean Accuracy using BaggingClassifier and K-Fold: 0.863
- Mean F1 Score using BaggingClassifier and K-Fold: 0.542
- Mean Log Loss using BaggingClassifier and K-Fold: 0.315
- Thời gian thực thi: 01:23:35

### 6.1.4 Using the method that makes changes to the data.

#### a) Oversampling using SMOTE
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_SMOTE(X, y, n_estimators=50, test_size=0.3, random_state=42):
    # Sử dụng SMOTE để oversample dữ liệu
    sm = SMOTE()
    X_sm, y_sm = sm.fit_resample(X_copy, y_copy)

    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    base_classifier = BaggingClassifier()
    start_time = time.time()

    # Train và validate
    X_train, X_valid, y_train, y_valid = train_test_split(X_sm, y_sm, test_size=test_size, stratify=y_sm, random_state=random_state)

    bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, bagging_classifier.predict_proba(X_valid))  # Tính log loss

    # Tính confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    accuracy_scores.append(accuracy)
    f1_scores.append(f1)
    log_losses.append(log_loss_val)
    confusion_matrices.append(cm)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' % accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % log_loss_val)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm để đánh giá mô hình
result_train_and_valid_split_SMOTE(X_copy, y_copy, n_estimators=50, test_size=0.3, random_state=42)

"""- Accuracy using BaggingClassifier: 0.853
- F1 Score using BaggingClassifier: 0.852
- Log Loss using BaggingClassifier: 0.324
- Thời gian thực thi: 00:29:17

#### b) Undersampling using NearMiss
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_Nearmiss(X, y, n_estimators=50, test_size=0.3, random_state=42):
    # Áp dụng NearMiss để xử lý mất cân bằng dữ liệu
    nm = NearMiss()
    X_res, y_res = nm.fit_resample(X_copy, y_copy)
    X_res.shape, y_res.shape

    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    base_classifier = BaggingClassifier()
    start_time = time.time()

    X_train, X_valid, y_train, y_valid = train_test_split(X_res, y_res, test_size= test_size, random_state= random_state)

    bagging_classifier = BaggingClassifier(base_classifier, n_estimators= n_estimators, random_state=random_state)

    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, bagging_classifier.predict_proba(X_valid))  # Tính log loss

    #Tính Confusion Matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' %accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % log_loss_val)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm để chạy
result_train_and_valid_split_Nearmiss(X_copy, y_copy, n_estimators=50, test_size=0.3, random_state=42)

"""- Accuracy using BaggingClassifier: 0.656
- F1 Score using BaggingClassifier: 0.655
- Log Loss using BaggingClassifier: 0.647
- Thời gian thực thi: 00:01:41

#### c) Random Oversampling
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_RandomOverSampler(X, y, n_estimators=50, test_size=0.3, random_state=42):
    # Áp dụng RandomOverSampler để xử lý mất cân bằng dữ liệu
    ros = RandomOverSampler()
    X_ros, y_ros = ros.fit_resample(X_copy, y_copy)
    X_ros.shape, y_ros.shape

    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    base_classifier = BaggingClassifier()
    start_time = time.time()

    X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=test_size, random_state=random_state)

    # Khởi tạo base_classifier

    bagging_classifier = BaggingClassifier(base_classifier, n_estimators= n_estimators, random_state=random_state)
    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, bagging_classifier.predict_proba(X_valid))  # Tính log loss

    # Tính Confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' % accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % log_loss_val)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm để chạy
result_train_and_valid_split_RandomOverSampler(X_copy, y_copy, n_estimators=50, test_size=0.3, random_state=42)

"""- Accuracy using BaggingClassifier: 0.900
- F1 Score using BaggingClassifier: 0.899
- Log Loss using BaggingClassifier: 0.232
- Thời gian thực thi: 00:15:55

So after comparisions, we found out that Random Oversampling is the best option.

### 6.1.5 OUR METHOD
"""

ros = RandomOverSampler()
X_ros, y_ros = ros.fit_resample(X, y)
X_ros.shape, y_ros.shape

# Train và kiểm tra mô hình trên dữ liệu đã xử lý
X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=0.3, random_state=42)

X_train.head()

X_valid.head()

y_train.head()

y_valid.head()

"""## 6.2 Machine learning (No Hyper Parameter Tuning)

### 6.2.1 Decision Tree
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_DecisionTree(X_train, X_valid, y_train, y_valid, random_state=42):
    start_time = time.time()

    # Bổ sung hyperparameters cho DecisionTreeClassifier
    clf = DecisionTreeClassifier(
        criterion='gini',  # Loại criterion: 'gini' hoặc 'entropy'
        splitter='best',  # Chọn 'best' hoặc 'random'
        max_depth=None,  # Độ sâu tối đa của cây
        min_samples_split=2,  # Số mẫu tối thiểu để chia một node nếu chưa đạt max_depth
        min_samples_leaf=1,  # Số mẫu tối thiểu trong các lá
        min_weight_fraction_leaf=0.0,
        max_features=None,  # Số tính năng tối đa để xem xét khi tìm kiếm phân chia tốt nhất
        random_state=random_state
    )
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using DecisionTreeClassifier: %.3f' % accuracy)
    print('F1 Score using DecisionTreeClassifier: %.3f' % f1)
    print('Log Loss using DecisionTreeClassifier: %.3f' % log_loss_val)
    print('AUC using DecisionTreeClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm
result_DecisionTree(X_train_copy, X_valid_copy, y_train_copy, y_valid_copy, random_state=42)

"""- Accuracy using DecisionTreeClassifier: 0.925
- F1 Score using DecisionTreeClassifier: 0.924
- Log Loss using DecisionTreeClassifier: 2.110
- AUC using DecisionTreeClassifier: 0.934
- Thời gian thực thi: 00:00:03

### 6.2.2 Logistic Regression
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_LogisticRegression(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình Logistic Regression và cài đặt các hyperparameters
    clf = LogisticRegression(
        penalty='l2',  # Loại penalty, có thể là 'l1' hoặc 'l2'
        C=1.0,  # Tham số điều chỉnh độ mạnh của regularization (1.0 là giá trị mặc định)
        solver='lbfgs',  # Thuật toán tối ưu hóa, có thể là 'lbfgs', 'liblinear', 'sag', 'saga'
        max_iter=100,  # Số lần lặp tối đa
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Logistic Regression')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    logloss = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using Logistic Regression: %.3f' % accuracy)
    print('F1 Score using Logistic Regression: %.3f' % f1)
    print('Log Loss using Logistic Regression: %.3f' % logloss)
    print('AUC using Logistic Regression: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_LogisticRegression() với dữ liệu đã chia sẵn
result_LogisticRegression(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using Logistic Regression: 0.788
- F1 Score using Logistic Regression: 0.782
- Log Loss using Logistic Regression: 0.435
- AUC using Logistic Regression: 0.842
- Thời gian thực thi: 00:00:02

### 6.2.3 Bagging Classifier
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_BaggingClassifier(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình BaggingClassifier và cài đặt các hyperparameters
    clf = BaggingClassifier(
        n_estimators=50,  # Số lượng cây quyết định trong ensemble
        random_state=42,  # Seed cho mô hình
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - BaggingClassifier')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    logloss = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using BaggingClassifier: %.3f' % accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % logloss)
    print('AUC using BaggingClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm với dữ liệu tương ứng
result_BaggingClassifier(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using BaggingClassifier: 0.923
- F1 Score using BaggingClassifier: 0.923
- Log Loss using BaggingClassifier: 0.287
- AUC using BaggingClassifier: 0.986
- Thời gian thực thi: 00:02:42

### 6.2.4 Random Forest
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_RandomForestClassifier(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình RandomForestClassifier và cài đặt các hyperparameters
    clf = RandomForestClassifier(
        n_estimators=50,  # Số lượng cây quyết định trong ensemble
        random_state=42,  # Seed cho mô hình
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - RandomForestClassifier')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    logloss = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using RandomForestClassifier: %.3f' % accuracy)
    print('F1 Score using RandomForestClassifier: %.3f' % f1)
    print('Log Loss using RandomForestClassifier: %.3f' % logloss)
    print('AUC using RandomForestClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_RandomForestClassifier() với dữ liệu đã chia sẵn
result_RandomForestClassifier(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using RandomForestClassifier: 0.920
- F1 Score using RandomForestClassifier: 0.920
- Log Loss using RandomForestClassifier: 0.348
- AUC using RandomForestClassifier: 0.982
- Thời gian thực thi: 00:01:01

### 6.2.5 XGBoost
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_XGBoostClassifier(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình XGBoostClassifier và cài đặt các hyperparameters
    clf = xgb.XGBClassifier(
        learning_rate=0.1,  # Tốc độ học
        nthread=-1,  # Số lượng luồng sử dụng (tự động sử dụng tất cả luồng có sẵn)
        max_depth=3,  # Độ sâu tối đa của cây
        colsample_bytree=1.0,  # Tỷ lệ mẫu đặc trưng khi xây dựng mỗi cây
        reg_lambda=1.0,  # Hệ số regularization L2
        reg_alpha=0.0  # Hệ số regularization L1
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Tính độ chính xác (accuracy)
    accuracy = accuracy_score(y_valid, y_pred)

    # Tính F1 score
    f1 = f1_score(y_valid, y_pred, average='macro')

    # Tính log loss
    y_pred_proba = clf.predict_proba(X_valid)
    logloss = log_loss(y_valid, y_pred_proba)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    # Tính ROC curve và AUC
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba[:, 1])
    auc_score = auc(fpr, tpr)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - XGBoostClassifier')
    plt.legend(loc='lower right')
    plt.show()

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    end_time = time.time()
    execution_time = end_time - start_time

    print('Accuracy using XGBoostClassifier: %.3f' % accuracy)
    print('F1 Score using XGBoostClassifier: %.3f' % f1)
    print('Log Loss using XGBoostClassifier: %.3f' % logloss)
    print('AUC using XGBoostClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_XGBoostClassifier() với dữ liệu đã chia sẵn
result_XGBoostClassifier(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using XGBoostClassifier: 0.790
- F1 Score using XGBoostClassifier: 0.785
- Log Loss using XGBoostClassifier: 0.431
- AUC using XGBoostClassifier: 0.845
- Thời gian thực thi: 00:00:05

### 6.2.6 Voting Classifier
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_SoftVoting(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo các mô hình con
    decision_tree = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None,
                                           min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
                                           max_features=None, random_state=42)
    logistic_regression = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100)
    bagging_classifier = BaggingClassifier(n_estimators=50, random_state=42)

    # Khởi tạo mô hình Soft Voting
    voting_clf = VotingClassifier(
        estimators=[('dt', decision_tree), ('lr', logistic_regression), ('bagging', bagging_classifier)],
        voting='soft'  # Soft Voting
    )

    # Huấn luyện mô hình Soft Voting
    voting_clf.fit(X_train, y_train)

    # Dự đoán xác suất cho từng lớp
    y_pred_proba = voting_clf.predict_proba(X_valid)

    # Dự đoán lớp (chọn lớp có xác suất cao nhất)
    y_pred = np.argmax(y_pred_proba, axis=1)

    # Tính độ chính xác (accuracy)
    accuracy = accuracy_score(y_valid, y_pred)

    # Tính F1 score
    f1 = f1_score(y_valid, y_pred, average='macro')

    # Tính log loss
    logloss = log_loss(y_valid, y_pred_proba)

    # Tính ROC curve và AUC
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba[:, 1])
    auc_score = auc(fpr, tpr)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Soft Voting')
    plt.legend(loc='lower right')
    plt.show()

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    end_time = time.time()
    execution_time = end_time - start_time

    print('Accuracy using Soft Voting: %.3f' % accuracy)
    print('F1 Score using Soft Voting: %.3f' % f1)
    print('Log Loss using Soft Voting: %.3f' % logloss)
    print('AUC using Soft Voting: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm với dữ liệu tương ứng
result_SoftVoting(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using Soft Voting: 0.924
- F1 Score using Soft Voting: 0.924
- Log Loss using Soft Voting: 0.231
- AUC using Soft Voting: 0.979
- Thời gian thực thi: 00:02:23

## 6.3 Hyper Parameter Tuning

### 6.3.1 Def Function
"""

def plot_confusion_matrix_and_roc_curves(model, X_valid, y_valid, y_pred):

    fig, axes = plt.subplots(1,2, figsize=(22,5))

    cm = confusion_matrix(y_valid, y_pred)
    group_names = ['True Neg','False Pos','False Neg','True Pos']
    group_counts = ['{0:0.0f}'.format(value) for value in
                    cm.flatten()]
    group_percentages = ['{0:.2%}'.format(value) for value in
                        cm.flatten()/np.sum(cm)]
    labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
            zip(group_names,group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)

    sns.heatmap(cm, ax = axes[0], annot=labels, fmt='',cmap='Blues')
    axes[0].set_title('Confusion Matrix', fontdict={'fontsize': 16, 'fontweight':'bold'})

    # predict probabilities
    pred_proba = model.predict_proba(X_valid)

    # roc curve for models
    fpr, tpr, thresh = roc_curve(y_valid, pred_proba[:,1], pos_label=1)

    # roc curve for tpr = fpr
    random_probs = [0 for i in range(len(y_valid))]
    p_fpr, p_tpr, _ = roc_curve(y_valid, random_probs, pos_label=1)

    plt.subplot(1, 2, 2)
    # plot roc curves
    plt.plot(fpr, tpr,linestyle='--',color='red', label = type(model).__name__)

    plt.plot(p_fpr, p_tpr, linestyle='-', color='blue')
    # title
    plt.title('ROC curve', fontdict={'fontsize': 16, 'fontweight':'bold'})
    # x label
    plt.xlabel('False Positive Rate', fontdict={'fontsize': 12})
    # y label
    plt.ylabel('True Positive rate', fontdict={'fontsize': 12})

    plt.legend(loc='best')
    plt.show()

def visualization(results_df, parameters):

    def shorten_param(param_name):
        if "__" in param_name:
            return param_name.rsplit("__", 1)[1]
        return param_name

    column_results = [f"param_{name}" for name in parameters.keys()]
    column_results += ["mean_test_score", "std_test_score", "rank_test_score"]

    results_df = results_df[column_results].sort_values("mean_test_score", ascending=False)
    results_df = results_df.rename(shorten_param, axis=1)

    for col in results_df.columns:
        if col == 'param_random_state':
            continue
        try:
            results_df[col] = results_df[col].astype(np.float64)
        except:
            continue

    fig = px.parallel_coordinates(
    results_df,
    color="mean_test_score",
    color_continuous_scale=px.colors.sequential.Viridis,
    title='Hyper Parameter Tuning',)
    fig.show()

def evaluation_metrics(name, independent_feature_length , y_pred, y_valid):

    metrics_dict = {}
    metrics_dict['Accuracy_Score'] = [accuracy_score(y_valid,y_pred)]  #Accuracy Score
    metrics_dict['Precision'] = [precision_score(y_valid,y_pred)] #Precision
    metrics_dict['Recall'] = [recall_score(y_valid,y_pred)] #Recall
    metrics_dict['F1_Score'] = [f1_score(y_valid,y_pred)] #F1 Score
    metrics_dict['ROC_AUC_Score'] = [roc_auc_score(y_valid, y_pred)] #ROC AUC Score
    metrics_dict['Log_Loss'] = [log_loss(y_valid, y_pred)] #Log Loss

    metrics_df = pd.DataFrame(metrics_dict)

    print(metrics_df)

def hyperparameter_tuning(X_train, y_train, model, parameters, tuning_model):

    if tuning_model == 'Halving_Randomized_Search_CV':
        tuned_model = HalvingRandomSearchCV(model, param_distributions=parameters, scoring="accuracy", n_jobs=-1, factor=3, cv=5)

    elif tuning_model == 'Randomized_Search_CV':
        tuned_model = RandomizedSearchCV(model, param_distributions=parameters, scoring='accuracy', cv=3, n_iter=50, n_jobs=-1)

    elif tuning_model == 'Grid_Search_CV':
        tuned_model = GridSearchCV(model, param_grid=parameters, scoring='accuracy', n_jobs=-1, cv=3)

    elif tuning_model == 'RandomSampler':
        sampler = RandomSampler(seed=42)  # Chọn seed tùy ý
        study = optuna.create_study(sampler=sampler, direction='maximize')
        optuna_model = optuna.integration.OptunaSearchCV(model, param_distributions=parameters, n_jobs=-1, cv=3, study=study, verbose=1)
        tuned_model = optuna_model

    elif tuning_model == 'TPE':
        sampler = TPESampler(seed=42)  # Chọn seed tùy ý
        study = optuna.create_study(sampler=sampler, direction='maximize')
        optuna_model = optuna.integration.OptunaSearchCV(model, param_distributions=parameters, n_jobs=-1, cv=3, study=study, verbose=1)
        tuned_model = optuna_model

    start_time = time.time()

    tuned_model.fit(X_train, y_train)

    stop_time = time.time()

    print('*****'*10+f'\nBest Score for {type(model).__name__} : {tuned_model.best_score_}','\n---')
    print(f'Best Parameters for {type(model).__name__} : {tuned_model.best_params_}\n'+'-----'*10)

    print('Elapsed Time:',time.strftime("%H:%M:%S", time.gmtime(stop_time - start_time)))
    print('======'*5)

    return tuned_model

def perform_ml_algorithm(X_train, X_valid, y_train, y_valid, model, parameters, tuning_model):
    print('-----'*10+f'\n{type(model).__name__}\n'+'-----'*10)

    model.fit(X_train, y_train)
    untuned_pred = model.predict(X_valid)

    # Evaluation Metrics before tuning
    print(f'\nEvaluation of {type(model).__name__} before tuning:\n'+'-----'*10)
    evaluation_metrics(type(model).__name__, len(list(X_train.columns)), untuned_pred, y_valid)

    print()
    plot_confusion_matrix_and_roc_curves(model, X_valid, y_valid, untuned_pred)

    # Hyper-parameter tuning
    tuned_model = hyperparameter_tuning(X_train, y_train, model, parameters, tuning_model)
    tuned_pred = tuned_model.predict(X_valid)

    # Evaluation Metrics after tuning
    print(f'\nEvaluation of {type(model).__name__} after tuning:\n'+'-----'*10)
    evaluation_metrics(type(model).__name__,len(list(X_train.columns)), tuned_pred, y_valid)

    print()
    plot_confusion_matrix_and_roc_curves(tuned_model.best_estimator_, X_valid, y_valid, tuned_pred)
    visualization(pd.DataFrame(tuned_model.cv_results_), parameters)

def ml_algorithm_implementation(df, model, parameters, tuning_model, feature_importance = False):

    if feature_importance == False:
        print('########'*8+'\n     <<<< '+f'Tuning Model: {tuning_model}'+' >>>>\n'+'********'*8)

    x = train.iloc[:,1:]
    y = train['Response']

    # Train Test Split
    ros = RandomOverSampler()
    X_ros, y_ros = ros.fit_resample(X, y)
    X_ros.shape, y_ros.shape

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=0.3, random_state=42)

    if feature_importance == True:
        model.fit(X_train, y_train)
        return X_train, y_train, model

    perform_ml_algorithm(X_train, X_valid, y_train, y_valid, model, parameters, tuning_model)

"""### 6.3.2 Main"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_bagging = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

Tuning_Method = 'Randomized_Search_CV'
parameters_bagging = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

Tuning_Method = 'Grid_Search_CV'
parameters_bagging = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

"""## 6.4 Machine Learning (Hyper Parameter Tuning)

### 6.4.1 Decision Tree
"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_decision_tree = {"splitter":["best","random"],
            "max_depth" : [None,5,7,9],
           "min_samples_leaf":[1,2,3,4,5],
           "min_weight_fraction_leaf":[0.0, 0.3,0.4,0.5],
           "max_features":["auto","log2","sqrt",None],
           "max_leaf_nodes":[None,30,40,50,60],
           'random_state':[42]}
ml_algorithm_implementation(train, DecisionTreeClassifier(), parameters_decision_tree, Tuning_Method, False)

"""### 6.4.2 Logistic Regression"""

# Set the tuning method
Tuning_Method = 'Halving_Randomized_Search_CV'

# Define the parameters for the logistic regression model
parameters_logistic = {'solver' : ['newton-cg', 'lbfgs', 'liblinear','sag','saga'],
                        'penalty' : ['l2'],
                        'C' : [100, 10, 1.0, 0.1, 0.01, 0.001],
                       'random_state':[42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, LogisticRegression(), parameters_logistic, Tuning_Method, False)

"""### 6.4.3 Bagging Classifier"""

Tuning_Method = 'Halving_Randomized_Search_CV'

parameters_bagging = {'n_estimators':[10, 100, 200, 400],
                      'random_state':[42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

"""### 6.4.4 Random Forest"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_random_forest = {'n_estimators': [100, 200, 400],
                            'max_depth': [3, 5, 7],
                            'random_state': [42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, RandomForestClassifier(), parameters_random_forest, Tuning_Method, False)

"""### 6.4.5 XGBoost"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_xgb = {'n_estimators': [100, 200, 400],
                  'max_depth': [3, 5, 7],
                  'learning_rate': [0.01, 0.1, 0.3],
                  'random_state': [42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, xgb.XGBClassifier(), parameters_xgb, Tuning_Method, False)

"""### 6.4.6 Voting Classifier"""

Tuning_Method = 'Halving_Randomized_Search_CV'
# Khởi tạo các mô hình con
decision_tree = DecisionTreeClassifier(max_depth=3)
logistic_regression = LogisticRegression()
bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)

# Khởi tạo mô hình Soft Voting
voting_clf = VotingClassifier(
    estimators=[('dt', decision_tree), ('lr', logistic_regression), ('bagging', bagging_classifier)],
    voting='soft'  # Chuyển sang Soft Voting
)

# Thiết lập hyperparameters bạn muốn tinh chỉnh (ở đây, chỉ có ví dụ cho decision_tree)
parameters_soft_voting = {
    'dt__max_depth': [3, 5, 7],
    'dt__min_samples_split': [2, 4, 6],
    # Thêm các hyperparameters cho các mô hình con khác nếu cần
}

# Run the Optuna hyperparameter tuning cho Soft Voting
ml_algorithm_implementation(train, voting_clf, parameters_soft_voting, Tuning_Method, False)

"""## 6.5 Extracting Feature Importance"""

def feature_plot(importances, X_train, y_train):

    # Display the five most important features
    indices = np.argsort(importances)[::-1]
    columns = X_train.columns.values[indices[:5]]
    values = importances[indices][:5]

    # Creat the plot
    fig = plt.figure(figsize = (9,5))
    plt.title("Normalized Weights for First Five Most Predictive Features", fontsize = 16)
    plt.bar(np.arange(5), values, width = 0.2, align="center", color = '#00A000', \
          label = "Feature Weight")
    plt.bar(np.arange(5) - 0.2, np.cumsum(values), width = 0.2, align = "center", color = '#00A0A0', \
          label = "Cumulative Feature Weight")
    plt.xticks(np.arange(5), columns)
    plt.xlim((-0.5, 4.5))
    plt.ylabel("Weight", fontsize = 12)
    plt.xlabel("Feature", fontsize = 12)

    plt.legend(loc = 'upper center')
    plt.tight_layout()
    plt.show()

def show_feature_importance():
    x_train, y_train, model = ml_algorithm_implementation(train, BaggingClassifier(n_estimators=200, random_state=42),
                                None, None, True)

    importances = np.mean([
        tree.feature_importances_ for tree in model.estimators_
        ], axis=0)
    feature_plot(importances, x_train, y_train)

show_feature_importance()



"""# VII. Machine Learning ( ratio 50-50)

## 7.1 Split Train.CSV into Train and Valid Dataset

### 7.1.1 Modeling Response Distrribution Percentage
"""

class_distribution = train['Response'].value_counts(normalize=True) * 100

colors = ['skyblue', 'lightgreen', 'lightcoral']

plt.figure(figsize=(8, 6))
bars = plt.bar(class_distribution.index, class_distribution.values, color=colors)
plt.xlabel('Class')
plt.ylabel('Percentage (%)')
plt.title('Response Distribution')

for bar in bars:
    yval = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, yval, f'{yval:.2f}%', ha='center', va='bottom')

plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 6))
plt.pie(class_distribution, labels=class_distribution.index, autopct='%1.1f%%', startangle=140, colors=colors)
plt.axis('equal')
plt.title('Response Distribution')
plt.show()

"""### 7.1.2 Seperate Target Column ( Response)"""

target_column = 'Response'
y = train[target_column]
X = train.drop(target_column, axis=1)

"""### 7.1.3 Using the method that makes no changes to the data.

- In all the tests below, to determine the optimal method for splitting the training and validation datasets for our 'train.csv' dataset, we utilize the BaggingClassifier to evaluate the accuracy of each method.
- After conducting the tests and obtaining the results, we will convert the code into markdown format if we achieve the highest accuracy percentages.
- We will use ratio all among the test below is 50:50

#### a) Method #1 (Normal way)
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_nmw(X, y, n_estimators=50, test_size=0.5, random_state=42):
    base_classifier = BaggingClassifier()
    start_time = time.time()

    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=test_size, random_state=random_state)

    # Initialize the base classifier as DecisionTreeClassifier
    bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')

    # Predict the probabilities for each class
    y_pred_proba = bagging_classifier.predict_proba(X_valid)

    # Calculate log loss
    logloss = log_loss(y_valid, y_pred_proba)

    # Tính confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' %accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % logloss)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

    return accuracy, f1, logloss

# Gọi hàm và lưu giá trị log loss vào biến
accuracy, f1, logloss = result_train_and_valid_split_nmw(X_copy, y_copy, n_estimators=50, test_size=0.5, random_state=42)

"""- Accuracy using BaggingClassifier: 0.863
- F1 Score using BaggingClassifier: 0.536
- Log Loss using BaggingClassifier: 0.316
- Thời gian thực thi: 00:06:53

#### b) Method #2 ( Stratified K-Fold way)
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_Stratified_KFold(X, y, n_estimators=50, n_splits=5, random_state=42):
    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Convert X and y to numpy arrays for indexing
    X = X.values
    y = y.values

    # Initialize the Stratified K-fold cross-validator
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    # Initialize the base classifier
    base_classifier = BaggingClassifier()

    start_time = time.time()  # Bắt đầu tính thời gian thực thi

    for train_index, test_index in skf.split(X, y):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Create and fit the bagging classifier
        bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
        bagging_classifier.fit(X_train, y_train)

        # Predict and calculate metrics
        y_pred = bagging_classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='macro')
        cm = confusion_matrix(y_test, y_pred)

        accuracy_scores.append(accuracy)
        f1_scores.append(f1)
        confusion_matrices.append(cm)

        # Tính và lưu log loss
        y_pred_proba = bagging_classifier.predict_proba(X_test)
        logloss = log_loss(y_test, y_pred_proba)
        log_losses.append(logloss)

    # Calculate mean accuracy and F1 score
    mean_accuracy = np.mean(accuracy_scores)
    mean_f1 = np.mean(f1_scores)

    # Calculate mean confusion matrix
    mean_confusion_matrix = np.mean(confusion_matrices, axis=0)

    # Calculate mean log loss
    mean_log_loss = np.mean(log_losses)

    end_time = time.time()  # Kết thúc tính thời gian thực thi
    execution_time = end_time - start_time

    # Vẽ mean confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(mean_confusion_matrix, annot=True, fmt='.2f', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Mean Confusion Matrix')
    plt.show()

    print('Mean Accuracy using BaggingClassifier and Stratified K-Fold: %.3f' % mean_accuracy)
    print('Mean F1 Score using BaggingClassifier and Stratified K-Fold: %.3f' % mean_f1)
    print('Mean Log Loss using BaggingClassifier and Stratified K-Fold: %.3f' % mean_log_loss)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_train_and_valid_split_Stratified_KFold để đánh giá mô hình
result_train_and_valid_split_Stratified_KFold(X_copy, y_copy, n_estimators=50, n_splits=5, random_state=42)

"""- Mean Accuracy using BaggingClassifier and Stratified K-Fold: 0.863
- Mean F1 Score using BaggingClassifier and Stratified K-Fold: 0.541
- Mean Log Loss using BaggingClassifier and Stratified K-Fold: 0.313
- Thời gian thực thi: 00:44:01

#### c) Method #3 (K-fold way)
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_KFold(X, y, n_estimators=50, n_splits=5, random_state=42):
    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Convert X and y to numpy arrays for indexing
    X = X.values
    y = y.values

    # Initialize the K-fold cross-validator
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=random_state)

    base_classifier = BaggingClassifier()
    start_time = time.time()  # Bắt đầu tính thời gian thực thi

    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]

        # Create and fit the bagging classifier
        bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
        bagging_classifier.fit(X_train, y_train)

        # Predict and calculate metrics
        y_pred = bagging_classifier.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred, average='macro')
        log_loss_val = log_loss(y_test, bagging_classifier.predict_proba(X_test))  # Tính log loss

        accuracy_scores.append(accuracy)
        f1_scores.append(f1)
        log_losses.append(log_loss_val)
        confusion_matrices.append(confusion_matrix(y_test, y_pred))

    # Calculate mean accuracy, F1 score, and log loss
    mean_accuracy = np.mean(accuracy_scores)
    mean_f1 = np.mean(f1_scores)
    mean_log_loss = np.mean(log_losses)  # Log loss trung bình
    mean_confusion_matrix = np.mean(confusion_matrices, axis=0)

    end_time = time.time()  # Kết thúc tính thời gian thực thi
    execution_time = end_time - start_time

    # Vẽ mean confusion matrix
    plt.figure(figsize=(8, 6))
    sns.heatmap(mean_confusion_matrix, annot=True, fmt='.2f', cmap='Blues', cbar=False)
    plt.xlabel('Predicted Labels')
    plt.ylabel('True Labels')
    plt.title('Mean Confusion Matrix')
    plt.show()

    print('Mean Accuracy using BaggingClassifier and K-Fold: %.3f' % mean_accuracy)
    print('Mean F1 Score using BaggingClassifier and K-Fold: %.3f' % mean_f1)
    print('Mean Log Loss using BaggingClassifier and K-Fold: %.3f' % mean_log_loss)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm để đánh giá mô hình
result_train_and_valid_split_KFold(X_copy, y_copy, n_estimators=50, n_splits=5, random_state=42)

"""- Mean Accuracy using BaggingClassifier and K-Fold: 0.863
- Mean F1 Score using BaggingClassifier and K-Fold: 0.542
- Mean Log Loss using BaggingClassifier and K-Fold: 0.312
- Thời gian thực thi: 00:40:03

### 7.1.4 Using the method that makes changes to the data.

#### a) Oversampling using SMOTE
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_SMOTE(X, y, n_estimators=50, test_size=0.5, random_state=42):
    # Sử dụng SMOTE để oversample dữ liệu
    sm = SMOTE()
    X_sm, y_sm = sm.fit_resample(X_copy, y_copy)

    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    base_classifier = BaggingClassifier()
    start_time = time.time()

    # Train và validate
    X_train, X_valid, y_train, y_valid = train_test_split(X_sm, y_sm, test_size=test_size, stratify=y_sm, random_state=random_state)

    bagging_classifier = BaggingClassifier(base_classifier, n_estimators=n_estimators, random_state=random_state)
    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, bagging_classifier.predict_proba(X_valid))  # Tính log loss

    # Tính confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    accuracy_scores.append(accuracy)
    f1_scores.append(f1)
    log_losses.append(log_loss_val)
    confusion_matrices.append(cm)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' % accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % log_loss_val)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm để đánh giá mô hình
result_train_and_valid_split_SMOTE(X_copy, y_copy, n_estimators=50, test_size=0.5, random_state=42)

"""- Accuracy using BaggingClassifier: 0.840
- F1 Score using BaggingClassifier: 0.839
- Log Loss using BaggingClassifier: 0.350
- Thời gian thực thi: 00:16:38

#### b) Undersampling using NearMiss
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_Nearmiss(X, y, n_estimators=50, test_size=0.5, random_state=42):
    # Áp dụng NearMiss để xử lý mất cân bằng dữ liệu
    nm = NearMiss()
    X_res, y_res = nm.fit_resample(X_copy, y_copy)
    X_res.shape, y_res.shape

    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    base_classifier = BaggingClassifier()
    start_time = time.time()

    X_train, X_valid, y_train, y_valid = train_test_split(X_res, y_res, test_size= test_size, random_state= random_state)

    bagging_classifier = BaggingClassifier(base_classifier, n_estimators= n_estimators, random_state=random_state)

    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, bagging_classifier.predict_proba(X_valid))  # Tính log loss

    #Tính Confusion Matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' %accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % log_loss_val)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm để chạy
result_train_and_valid_split_Nearmiss(X_copy, y_copy, n_estimators=50, test_size=0.5, random_state=42)

"""- Accuracy using BaggingClassifier: 0.653
- F1 Score using BaggingClassifier: 0.652
- Log Loss using BaggingClassifier: 0.650
- Thời gian thực thi: 00:01:30

#### c) Random Oversampling
"""

# Checkpoint
train_copy= train.copy()
target_column = 'Response'
y_copy = y.copy()
X_copy = X.copy()
#READY TO USE :)

def result_train_and_valid_split_RandomOverSampler(X, y, n_estimators=50, test_size=0.5, random_state=42):
    # Áp dụng RandomOverSampler để xử lý mất cân bằng dữ liệu
    ros = RandomOverSampler()
    X_ros, y_ros = ros.fit_resample(X_copy, y_copy)
    X_ros.shape, y_ros.shape

    accuracy_scores = []
    f1_scores = []
    log_losses = []  # Thêm biến để lưu log loss
    confusion_matrices = []

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    base_classifier = BaggingClassifier()
    start_time = time.time()

    X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=test_size, random_state=random_state)

    # Khởi tạo base_classifier

    bagging_classifier = BaggingClassifier(base_classifier, n_estimators= n_estimators, random_state=random_state)
    bagging_classifier.fit(X_train, y_train)

    y_pred = bagging_classifier.predict(X_valid)

    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, bagging_classifier.predict_proba(X_valid))  # Tính log loss

    # Tính Confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    print('Accuracy using BaggingClassifier: %.3f' % accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % log_loss_val)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm để chạy
result_train_and_valid_split_RandomOverSampler(X_copy, y_copy, n_estimators=50, test_size=0.5, random_state=42)

"""- Accuracy using BaggingClassifier: 0.877
- F1 Score using BaggingClassifier: 0.876
- Log Loss using BaggingClassifier: 0.274
- Thời gian thực thi: 00:11:44

So after comparisions, we found out that Random Oversampling is the best option.

### 7.1.5. OUR METHOD
"""

ros = RandomOverSampler()
X_ros, y_ros = ros.fit_resample(X, y)
X_ros.shape, y_ros.shape

# Train và kiểm tra mô hình trên dữ liệu đã xử lý
X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=0.5, random_state=42)

X_train.head()

X_valid.head()

y_train.head()

y_valid.head()

"""## 7.2 Machine learning (No Hyper Parameter Tuning)

### 7.2.1 Decision Tree
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_DecisionTree(X_train, X_valid, y_train, y_valid, random_state=42):
    start_time = time.time()

    # Bổ sung hyperparameters cho DecisionTreeClassifier
    clf = DecisionTreeClassifier(
        criterion='gini',  # Loại criterion: 'gini' hoặc 'entropy'
        splitter='best',  # Chọn 'best' hoặc 'random'
        max_depth=None,  # Độ sâu tối đa của cây
        min_samples_split=2,  # Số mẫu tối thiểu để chia một node nếu chưa đạt max_depth
        min_samples_leaf=1,  # Số mẫu tối thiểu trong các lá
        min_weight_fraction_leaf=0.0,
        max_features=None,  # Số tính năng tối đa để xem xét khi tìm kiếm phân chia tốt nhất
        random_state=random_state
    )
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    log_loss_val = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using DecisionTreeClassifier: %.3f' % accuracy)
    print('F1 Score using DecisionTreeClassifier: %.3f' % f1)
    print('Log Loss using DecisionTreeClassifier: %.3f' % log_loss_val)
    print('AUC using DecisionTreeClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm
result_DecisionTree(X_train_copy, X_valid_copy, y_train_copy, y_valid_copy, random_state=42)

"""- Accuracy using DecisionTreeClassifier: 0.905
- F1 Score using DecisionTreeClassifier: 0.904
- Log Loss using DecisionTreeClassifier: 2.851
- AUC using DecisionTreeClassifier: 0.913
- Thời gian thực thi: 00:00:03

### 7.2.2 Logistic Regression
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_LogisticRegression(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình Logistic Regression và cài đặt các hyperparameters
    clf = LogisticRegression(
        penalty='l2',  # Loại penalty, có thể là 'l1' hoặc 'l2'
        C=1.0,  # Tham số điều chỉnh độ mạnh của regularization (1.0 là giá trị mặc định)
        solver='lbfgs',  # Thuật toán tối ưu hóa, có thể là 'lbfgs', 'liblinear', 'sag', 'saga'
        max_iter=100,  # Số lần lặp tối đa
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Logistic Regression')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    logloss = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using Logistic Regression: %.3f' % accuracy)
    print('F1 Score using Logistic Regression: %.3f' % f1)
    print('Log Loss using Logistic Regression: %.3f' % logloss)
    print('AUC using Logistic Regression: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_LogisticRegression() với dữ liệu đã chia sẵn
result_LogisticRegression(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using Logistic Regression: 0.787
- F1 Score using Logistic Regression: 0.782
- Log Loss using Logistic Regression: 0.435
- AUC using Logistic Regression: 0.841
- Thời gian thực thi: 00:00:02

### 7.2.3 Bagging Classifier
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_BaggingClassifier(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình BaggingClassifier và cài đặt các hyperparameters
    clf = BaggingClassifier(
        n_estimators=50,  # Số lượng cây quyết định trong ensemble
        random_state=42,  # Seed cho mô hình
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - BaggingClassifier')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    logloss = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using BaggingClassifier: %.3f' % accuracy)
    print('F1 Score using BaggingClassifier: %.3f' % f1)
    print('Log Loss using BaggingClassifier: %.3f' % logloss)
    print('AUC using BaggingClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm với dữ liệu tương ứng
result_BaggingClassifier(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using BaggingClassifier: 0.901
- F1 Score using BaggingClassifier: 0.901
- Log Loss using BaggingClassifier: 0.339
- AUC using BaggingClassifier: 0.976
- Thời gian thực thi: 00:01:45

### 7.2.4 Random Forest
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_RandomForestClassifier(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình RandomForestClassifier và cài đặt các hyperparameters
    clf = RandomForestClassifier(
        n_estimators=50,  # Số lượng cây quyết định trong ensemble
        random_state=42,  # Seed cho mô hình
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Dự đoán xác suất cho lớp positive (class=1)
    y_pred_proba = clf.predict_proba(X_valid)[:, 1]

    # Tính FPR, TPR và ngưỡng (thresholds) sử dụng cho ROC curve
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc(fpr, tpr)))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - RandomForestClassifier')
    plt.legend(loc='lower right')
    plt.show()

    # Tính AUC
    auc_score = auc(fpr, tpr)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    end_time = time.time()
    execution_time = end_time - start_time

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    # Tính các chỉ số đánh giá
    accuracy = accuracy_score(y_valid, y_pred)
    f1 = f1_score(y_valid, y_pred, average='macro')
    logloss = log_loss(y_valid, clf.predict_proba(X_valid))

    print('Accuracy using RandomForestClassifier: %.3f' % accuracy)
    print('F1 Score using RandomForestClassifier: %.3f' % f1)
    print('Log Loss using RandomForestClassifier: %.3f' % logloss)
    print('AUC using RandomForestClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_RandomForestClassifier() với dữ liệu đã chia sẵn
result_RandomForestClassifier(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using RandomForestClassifier: 0.899
- F1 Score using RandomForestClassifier: 0.899
- Log Loss using RandomForestClassifier: 0.424
- AUC using RandomForestClassifier: 0.969
- Thời gian thực thi: 00:00:43

### 7.2.5 XGBoost
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_XGBoostClassifier(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo mô hình XGBoostClassifier và cài đặt các hyperparameters
    clf = xgb.XGBClassifier(
        learning_rate=0.1,  # Tốc độ học
        nthread=-1,  # Số lượng luồng sử dụng (tự động sử dụng tất cả luồng có sẵn)
        max_depth=3,  # Độ sâu tối đa của cây
        colsample_bytree=1.0,  # Tỷ lệ mẫu đặc trưng khi xây dựng mỗi cây
        reg_lambda=1.0,  # Hệ số regularization L2
        reg_alpha=0.0  # Hệ số regularization L1
    )

    # Huấn luyện mô hình
    clf.fit(X_train, y_train)

    # Dự đoán
    y_pred = clf.predict(X_valid)

    # Tính độ chính xác (accuracy)
    accuracy = accuracy_score(y_valid, y_pred)

    # Tính F1 score
    f1 = f1_score(y_valid, y_pred, average='macro')

    # Tính log loss
    y_pred_proba = clf.predict_proba(X_valid)
    logloss = log_loss(y_valid, y_pred_proba)

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    # Tính ROC curve và AUC
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba[:, 1])
    auc_score = auc(fpr, tpr)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - XGBoostClassifier')
    plt.legend(loc='lower right')
    plt.show()

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    end_time = time.time()
    execution_time = end_time - start_time

    print('Accuracy using XGBoostClassifier: %.3f' % accuracy)
    print('F1 Score using XGBoostClassifier: %.3f' % f1)
    print('Log Loss using XGBoostClassifier: %.3f' % logloss)
    print('AUC using XGBoostClassifier: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Sử dụng hàm result_XGBoostClassifier() với dữ liệu đã chia sẵn
result_XGBoostClassifier(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using XGBoostClassifier: 0.790
- F1 Score using XGBoostClassifier: 0.785
- Log Loss using XGBoostClassifier: 0.432
- AUC using XGBoostClassifier: 0.845
- Thời gian thực thi: 00:00:04

### 7.2.6 Voting Classifier
"""

# Checkpoint
X_train_copy = X_train.copy()
X_valid_copy = X_valid.copy()
y_train_copy = y_train.copy()
y_valid_copy = y_valid.copy()
#READY TO USE :)

def result_SoftVoting(X_train, y_train, X_valid, y_valid):
    start_time = time.time()

    # Khởi tạo các mô hình con
    decision_tree = DecisionTreeClassifier(criterion='gini', splitter='best', max_depth=None,
                                           min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0,
                                           max_features=None, random_state=42)
    logistic_regression = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=100)
    bagging_classifier = BaggingClassifier(n_estimators=50, random_state=42)

    # Khởi tạo mô hình Soft Voting
    voting_clf = VotingClassifier(
        estimators=[('dt', decision_tree), ('lr', logistic_regression), ('bagging', bagging_classifier)],
        voting='soft'  # Soft Voting
    )

    # Huấn luyện mô hình Soft Voting
    voting_clf.fit(X_train, y_train)

    # Dự đoán xác suất cho từng lớp
    y_pred_proba = voting_clf.predict_proba(X_valid)

    # Dự đoán lớp (chọn lớp có xác suất cao nhất)
    y_pred = np.argmax(y_pred_proba, axis=1)

    # Tính độ chính xác (accuracy)
    accuracy = accuracy_score(y_valid, y_pred)

    # Tính F1 score
    f1 = f1_score(y_valid, y_pred, average='macro')

    # Tính log loss
    logloss = log_loss(y_valid, y_pred_proba)

    # Tính ROC curve và AUC
    fpr, tpr, thresholds = roc_curve(y_valid, y_pred_proba[:, 1])
    auc_score = auc(fpr, tpr)

    # Vẽ ROC curve
    plt.figure()
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = {:.2f})'.format(auc_score))
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve - Soft Voting')
    plt.legend(loc='lower right')
    plt.show()

    # Tính và vẽ confusion matrix
    cm = confusion_matrix(y_valid, y_pred)

    # Vẽ confusion matrix bằng seaborn
    plt.figure()
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.title('Confusion Matrix')
    plt.show()

    end_time = time.time()
    execution_time = end_time - start_time

    print('Accuracy using Soft Voting: %.3f' % accuracy)
    print('F1 Score using Soft Voting: %.3f' % f1)
    print('Log Loss using Soft Voting: %.3f' % logloss)
    print('AUC using Soft Voting: %.3f' % auc_score)
    print("Thời gian thực thi:", time.strftime("%H:%M:%S", time.gmtime(execution_time)))

# Gọi hàm với dữ liệu tương ứng
result_SoftVoting(X_train_copy, y_train_copy, X_valid_copy, y_valid_copy)

"""- Accuracy using Soft Voting: 0.905
- F1 Score using Soft Voting: 0.904
- Log Loss using Soft Voting: 0.261
- AUC using Soft Voting: 0.968
- Thời gian thực thi: 00:01:46

## 7.3 Hyper Parameter Tuning

### 7.3.1 Def Function
"""

def plot_confusion_matrix_and_roc_curves(model, X_valid, y_valid, y_pred):

    fig, axes = plt.subplots(1,2, figsize=(22,5))

    cm = confusion_matrix(y_valid, y_pred)
    group_names = ['True Neg','False Pos','False Neg','True Pos']
    group_counts = ['{0:0.0f}'.format(value) for value in
                    cm.flatten()]
    group_percentages = ['{0:.2%}'.format(value) for value in
                        cm.flatten()/np.sum(cm)]
    labels = [f'{v1}\n{v2}\n{v3}' for v1, v2, v3 in
            zip(group_names,group_counts,group_percentages)]
    labels = np.asarray(labels).reshape(2,2)

    sns.heatmap(cm, ax = axes[0], annot=labels, fmt='',cmap='Blues')
    axes[0].set_title('Confusion Matrix', fontdict={'fontsize': 16, 'fontweight':'bold'})

    # predict probabilities
    pred_proba = model.predict_proba(X_valid)

    # roc curve for models
    fpr, tpr, thresh = roc_curve(y_valid, pred_proba[:,1], pos_label=1)

    # roc curve for tpr = fpr
    random_probs = [0 for i in range(len(y_valid))]
    p_fpr, p_tpr, _ = roc_curve(y_valid, random_probs, pos_label=1)

    plt.subplot(1, 2, 2)
    # plot roc curves
    plt.plot(fpr, tpr,linestyle='--',color='red', label = type(model).__name__)

    plt.plot(p_fpr, p_tpr, linestyle='-', color='blue')
    # title
    plt.title('ROC curve', fontdict={'fontsize': 16, 'fontweight':'bold'})
    # x label
    plt.xlabel('False Positive Rate', fontdict={'fontsize': 12})
    # y label
    plt.ylabel('True Positive rate', fontdict={'fontsize': 12})

    plt.legend(loc='best')
    plt.show()

def visualization(results_df, parameters):

    def shorten_param(param_name):
        if "__" in param_name:
            return param_name.rsplit("__", 1)[1]
        return param_name

    column_results = [f"param_{name}" for name in parameters.keys()]
    column_results += ["mean_test_score", "std_test_score", "rank_test_score"]

    results_df = results_df[column_results].sort_values("mean_test_score", ascending=False)
    results_df = results_df.rename(shorten_param, axis=1)

    for col in results_df.columns:
        if col == 'param_random_state':
            continue
        try:
            results_df[col] = results_df[col].astype(np.float64)
        except:
            continue

    fig = px.parallel_coordinates(
    results_df,
    color="mean_test_score",
    color_continuous_scale=px.colors.sequential.Viridis,
    title='Hyper Parameter Tuning',)
    fig.show()

def evaluation_metrics(name, independent_feature_length , y_pred, y_valid):

    metrics_dict = {}
    metrics_dict['Accuracy_Score'] = [accuracy_score(y_valid,y_pred)]  #Accuracy Score
    metrics_dict['Precision'] = [precision_score(y_valid,y_pred)] #Precision
    metrics_dict['Recall'] = [recall_score(y_valid,y_pred)] #Recall
    metrics_dict['F1_Score'] = [f1_score(y_valid,y_pred)] #F1 Score
    metrics_dict['ROC_AUC_Score'] = [roc_auc_score(y_valid, y_pred)] #ROC AUC Score
    metrics_dict['Log_Loss'] = [log_loss(y_valid, y_pred)] #Log Loss

    metrics_df = pd.DataFrame(metrics_dict)

    print(metrics_df)

def hyperparameter_tuning(X_train, y_train, model, parameters, tuning_model):

    if tuning_model == 'Halving_Randomized_Search_CV':
        tuned_model = HalvingRandomSearchCV(model, param_distributions=parameters, scoring="accuracy", n_jobs=-1, factor=3, cv=5)

    elif tuning_model == 'Randomized_Search_CV':
        tuned_model = RandomizedSearchCV(model, param_distributions=parameters, scoring='accuracy', cv=3, n_iter=50, n_jobs=-1)

    elif tuning_model == 'Grid_Search_CV':
        tuned_model = GridSearchCV(model, param_grid=parameters, scoring='accuracy', n_jobs=-1, cv=3)

    elif tuning_model == 'RandomSampler':
        sampler = RandomSampler(seed=42)  # Chọn seed tùy ý
        study = optuna.create_study(sampler=sampler, direction='maximize')
        optuna_model = optuna.integration.OptunaSearchCV(model, param_distributions=parameters, n_jobs=-1, cv=3, study=study, verbose=1)
        tuned_model = optuna_model

    elif tuning_model == 'TPE':
        sampler = TPESampler(seed=42)  # Chọn seed tùy ý
        study = optuna.create_study(sampler=sampler, direction='maximize')
        optuna_model = optuna.integration.OptunaSearchCV(model, param_distributions=parameters, n_jobs=-1, cv=3, study=study, verbose=1)
        tuned_model = optuna_model

    start_time = time.time()

    tuned_model.fit(X_train, y_train)

    stop_time = time.time()

    print('*****'*10+f'\nBest Score for {type(model).__name__} : {tuned_model.best_score_}','\n---')
    print(f'Best Parameters for {type(model).__name__} : {tuned_model.best_params_}\n'+'-----'*10)

    print('Elapsed Time:',time.strftime("%H:%M:%S", time.gmtime(stop_time - start_time)))
    print('======'*5)

    return tuned_model

def perform_ml_algorithm(X_train, X_valid, y_train, y_valid, model, parameters, tuning_model):
    print('-----'*10+f'\n{type(model).__name__}\n'+'-----'*10)

    model.fit(X_train, y_train)
    untuned_pred = model.predict(X_valid)

    # Evaluation Metrics before tuning
    print(f'\nEvaluation of {type(model).__name__} before tuning:\n'+'-----'*10)
    evaluation_metrics(type(model).__name__, len(list(X_train.columns)), untuned_pred, y_valid)

    print()
    plot_confusion_matrix_and_roc_curves(model, X_valid, y_valid, untuned_pred)

    # Hyper-parameter tuning
    tuned_model = hyperparameter_tuning(X_train, y_train, model, parameters, tuning_model)
    tuned_pred = tuned_model.predict(X_valid)

    # Evaluation Metrics after tuning
    print(f'\nEvaluation of {type(model).__name__} after tuning:\n'+'-----'*10)
    evaluation_metrics(type(model).__name__,len(list(X_train.columns)), tuned_pred, y_valid)

    print()
    plot_confusion_matrix_and_roc_curves(tuned_model.best_estimator_, X_valid, y_valid, tuned_pred)
    visualization(pd.DataFrame(tuned_model.cv_results_), parameters)

def ml_algorithm_implementation(df, model, parameters, tuning_model, feature_importance = False):

    if feature_importance == False:
        print('########'*8+'\n     <<<< '+f'Tuning Model: {tuning_model}'+' >>>>\n'+'********'*8)

    x = train.iloc[:,1:]
    y = train['Response']

    # Train Test Split
    ros = RandomOverSampler()
    X_ros, y_ros = ros.fit_resample(X, y)
    X_ros.shape, y_ros.shape

    # Train và kiểm tra mô hình trên dữ liệu đã xử lý
    X_train, X_valid, y_train, y_valid = train_test_split(X_ros, y_ros, test_size=0.5, random_state=42)

    if feature_importance == True:
        model.fit(X_train, y_train)
        return X_train, y_train, model

    perform_ml_algorithm(X_train, X_valid, y_train, y_valid, model, parameters, tuning_model)

"""### 7.3.2 Main"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_bagging = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

Tuning_Method = 'Randomized_Search_CV'
parameters_bagging = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

Tuning_Method = 'Grid_Search_CV'
parameters_bagging = {
    'n_estimators': [10, 100, 200, 400],
    'max_samples': [0.5, 0.7, 1.0],
    'random_state': [42]
}

ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

"""## 7.4 Machine Learning (Hyper Parameter Tuning)

### 7.4.1 Decision Tree
"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_decision_tree = {"splitter":["best","random"],
            "max_depth" : [None,5,7,9],
           "min_samples_leaf":[1,2,3,4,5],
           "min_weight_fraction_leaf":[0.0, 0.3,0.4,0.5],
           "max_features":["auto","log2","sqrt",None],
           "max_leaf_nodes":[None,30,40,50,60],
           'random_state':[42]}
ml_algorithm_implementation(train, DecisionTreeClassifier(), parameters_decision_tree, Tuning_Method, False)

"""### 7.4.2 Logistic Regression"""

# Set the tuning method
Tuning_Method = 'Halving_Randomized_Search_CV'

# Define the parameters for the logistic regression model
parameters_logistic = {'solver' : ['newton-cg', 'lbfgs', 'liblinear','sag','saga'],
                        'penalty' : ['l2'],
                        'C' : [100, 10, 1.0, 0.1, 0.01, 0.001],
                       'random_state':[42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, LogisticRegression(), parameters_logistic, Tuning_Method, False)

"""### 7.4.3 Bagging Classifier"""

Tuning_Method = 'Halving_Randomized_Search_CV'

parameters_bagging = {'n_estimators':[10, 100, 200, 400],
                      'random_state':[42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, BaggingClassifier(), parameters_bagging, Tuning_Method, False)

"""### 7.4.4 Random Forest"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_random_forest = {'n_estimators': [100, 200, 400],
                            'max_depth': [3, 5, 7],
                            'random_state': [42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, RandomForestClassifier(), parameters_random_forest, Tuning_Method, False)

"""### 7.4.5 XGBoost"""

Tuning_Method = 'Halving_Randomized_Search_CV'
parameters_xgb = {'n_estimators': [100, 200, 400],
                  'max_depth': [3, 5, 7],
                  'learning_rate': [0.01, 0.1, 0.3],
                  'random_state': [42]}

# Run the Optuna hyperparameter tuning
ml_algorithm_implementation(train, xgb.XGBClassifier(), parameters_xgb, Tuning_Method, False)

"""### 7.4.6 Voting Classifier"""

Tuning_Method = 'Halving_Randomized_Search_CV'
# Khởi tạo các mô hình con
decision_tree = DecisionTreeClassifier(max_depth=3)
logistic_regression = LogisticRegression()
bagging_classifier = BaggingClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)

# Khởi tạo mô hình Soft Voting
voting_clf = VotingClassifier(
    estimators=[('dt', decision_tree), ('lr', logistic_regression), ('bagging', bagging_classifier)],
    voting='soft'  # Chuyển sang Soft Voting
)

# Thiết lập hyperparameters bạn muốn tinh chỉnh (ở đây, chỉ có ví dụ cho decision_tree)
parameters_soft_voting = {
    'dt__max_depth': [3, 5, 7],
    'dt__min_samples_split': [2, 4, 6],
    # Thêm các hyperparameters cho các mô hình con khác nếu cần
}

# Run the Optuna hyperparameter tuning cho Soft Voting
ml_algorithm_implementation(train, voting_clf, parameters_soft_voting, Tuning_Method, False)

"""## 7.5 Extracting Feature Importance"""

def feature_plot(importances, X_train, y_train):

    # Display the five most important features
    indices = np.argsort(importances)[::-1]
    columns = X_train.columns.values[indices[:5]]
    values = importances[indices][:5]

    # Creat the plot
    fig = plt.figure(figsize = (9,5))
    plt.title("Normalized Weights for First Five Most Predictive Features", fontsize = 16)
    plt.bar(np.arange(5), values, width = 0.2, align="center", color = '#00A000', \
          label = "Feature Weight")
    plt.bar(np.arange(5) - 0.2, np.cumsum(values), width = 0.2, align = "center", color = '#00A0A0', \
          label = "Cumulative Feature Weight")
    plt.xticks(np.arange(5), columns)
    plt.xlim((-0.5, 4.5))
    plt.ylabel("Weight", fontsize = 12)
    plt.xlabel("Feature", fontsize = 12)

    plt.legend(loc = 'upper center')
    plt.tight_layout()
    plt.show()

def show_feature_importance():
    x_train, y_train, model = ml_algorithm_implementation(train, BaggingClassifier(n_estimators=200, random_state=42),
                                None, None, True)

    importances = np.mean([
        tree.feature_importances_ for tree in model.estimators_
        ], axis=0)
    feature_plot(importances, x_train, y_train)

show_feature_importance()

"""# IX. SAVE MODEL"""

filename = 'rf_model.sav'
pickle.dump(model, open(filename, 'wb'))

filename = 'rf_model.sav'

rf_load = pickle.load(open(filename, 'rb'))